{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":168.614017,"end_time":"2023-10-30T00:47:50.427740","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-10-30T00:45:01.813723","version":"2.4.0"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":57891,"databundleVersionId":7056235,"sourceType":"competition"}],"dockerImageVersionId":30617,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU"},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":["# NS\n","#optiver-ver (for ensemble)\n","\n","#optiver-ver-nn-modeling/inference-v1\n","#optiver-ver-rnn-modeling/inference-v1\n","#optiver-ver-lgb-modeling/inference-v1\n","\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.linear_model import LinearRegression\n","# import lightgbm as lgb\n","# import xgboost as xgb\n","from sklearn.linear_model import Ridge\n","from sklearn.metrics import mean_absolute_error\n","import matplotlib.pyplot as plt\n","from scipy.stats import hmean\n","\n","from sklearn.ensemble import HistGradientBoostingRegressor\n","import itertools\n","import pickle\n","import joblib\n","from itertools import combinations\n","from tqdm import tqdm\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Concatenate, GaussianNoise\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import EarlyStopping\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.experimental import CosineDecay\n","from tensorflow.keras.optimizers.schedules import ExponentialDecay\n","from tensorflow.keras.layers import concatenate,Dropout\n","import pickle\n","from tensorflow.keras.models import load_model\n","import os\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import Huber\n","from tensorflow.keras.metrics import MeanAbsoluteError\n","from tensorflow.keras.callbacks import Callback\n","import random\n","from tensorflow.keras.layers import Input, Embedding, Lambda, Reshape, LSTM, Dense, BatchNormalization, Dropout, concatenate\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.layers import ZeroPadding1D\n","from tensorflow.keras.layers import Conv1D\n","from tensorflow.keras.layers import RepeatVector\n","\n","from tensorflow.keras.layers import Input, Embedding, Lambda, Reshape, LSTM, Dense, BatchNormalization, Dropout, concatenate\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.layers import ZeroPadding1D, Activation\n","from tensorflow.keras.layers import Conv1D\n","from tensorflow.keras.layers import RepeatVector\n","from tensorflow.keras.layers import MaxPooling1D, AveragePooling1D\n","from tensorflow.keras.layers import Add\n","\n","#---------------------------------------------------------------------- setup dashboard ------------------------------------------------------------\n","\n","kaggle              = False\n","is_inference        = False\n","load_models         = False\n","run_pipeline        = True\n","train_models        = True\n","is_lgb              = False    #1\n","is_nn               = False    #2\n","is_rnn              = True   #3\n","simulation          = False\n","online_learning     = False\n","\n","manage_memory       = False\n","memory_threshold    = 24576  #24GB\n","\n","ens_models          = [1.0,1.0,1.0]\n","\n","#public-validation\n","# dates_train = [0,390]\n","# dates_test = [391,480]\n","\n","#full-inference\n","dates_train = [0,480]\n","dates_test = [-1,-1]\n","\n","num_models ={'lgb':1,'nn':1,'rnn':2}\n","\n","\n","if kaggle:\n","    train_path = \"/kaggle/input/optiver-trading-at-the-close/train.csv\"\n","    models_path = \"/kaggle/input/optiver-258-rnn-inference-models/\"\n","else:\n","    models_path = \"optiver-inference-models/\"\n","    train_path = \"train.csv\"\n","\n","if dates_train[1]!=480:\n","    models_path = \"vals-258/\"\n","#---------------------------------------------------------------------- setup dashboard ------------------------------------------------------------\n","\n","simulation_path = \"vals-258/\""],"metadata":{"papermill":{"duration":13.419877,"end_time":"2023-10-30T00:45:18.598788","exception":false,"start_time":"2023-10-30T00:45:05.178911","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-13T01:39:53.936112Z","iopub.execute_input":"2023-12-13T01:39:53.936954Z","iopub.status.idle":"2023-12-13T01:40:09.425279Z","shell.execute_reply.started":"2023-12-13T01:39:53.936915Z","shell.execute_reply":"2023-12-13T01:40:09.424255Z"},"trusted":true,"id":"AHqylKuN78oc","executionInfo":{"status":"ok","timestamp":1702477248946,"user_tz":300,"elapsed":3773,"user":{"displayName":"Nima Shahbazi","userId":"15346950202304736225"}}},"execution_count":1,"outputs":[],"id":"AHqylKuN78oc"},{"cell_type":"code","source":["#@title params dashboard\n","lgb_params = {\n","            'learning_rate'     : 0.005,  #0.005,0.05\n","            'max_depth'         : 14, #14\n","            'n_estimators'      : 5000,\n","            'num_leaves'        : 1023,    #511,31,1023\n","            'objective'         : 'mae',\n","            'subsample'         : .2,\n","            'colsample_bytree'  : .3,\n","            'num_threads'       : 32,\n","            'device'            : 'gpu',\n","            # 'reg_alpha'         : 0.1,\n","            # 'reg_lambda'        : 4,\n","        }\n","\n","nn_ep          = 100\n","nn_lr          = 0.001\n","nn_bs          = 2**15\n","\n","\n","rnn_ep         = 100\n","rnn_lr         = 0.001\n","rnn_bs         = 2**12\n","window_size    = 3"],"metadata":{"execution":{"iopub.status.busy":"2023-12-13T01:40:09.427019Z","iopub.execute_input":"2023-12-13T01:40:09.427597Z","iopub.status.idle":"2023-12-13T01:40:09.432891Z","shell.execute_reply.started":"2023-12-13T01:40:09.427562Z","shell.execute_reply":"2023-12-13T01:40:09.432078Z"},"trusted":true,"id":"0K9ZMhVz78of","executionInfo":{"status":"ok","timestamp":1702477248946,"user_tz":300,"elapsed":4,"user":{"displayName":"Nima Shahbazi","userId":"15346950202304736225"}}},"execution_count":2,"outputs":[],"id":"0K9ZMhVz78of"},{"cell_type":"code","source":["#@title read train\n","\n","colab = True\n","if colab:\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  train_path = '/content/drive/My Drive/optiver/train.csv'\n","  models_path = \"/content/drive/My Drive/optiver/optiver-258-rnn-inference-74/\"\n","else:\n","    if not kaggle:\n","        from public_timeseries_testing_util import MockApi\n","\n","pd.set_option('mode.chained_assignment', None)\n","\n","\n","def convert_price_cols_float32(df):\n","\n","    # Columns containing 'price'\n","    price_columns = [col for col in df.columns if 'price' in col]\n","    df[price_columns] = df[price_columns].astype('float32')\n","\n","    # Columns containing 'wap'\n","    wap_columns = [col for col in df.columns if 'wap' in col]\n","    df[wap_columns] = df[wap_columns].astype('float32')\n","\n","    return df\n","\n","train = pd.read_csv(train_path).drop(['row_id', 'time_id'], axis = 1)\n","nan_count = train['target'].isna().sum()\n","print(f\"The 'target' column has {nan_count} NaN values.\")\n","\n","target_median = train['target'].median()\n","train['target'].fillna(target_median, inplace=True)\n","\n","print(f\"converting prices columns to float32 values.\")\n","train = convert_price_cols_float32(train)\n","# ----------------------------- Reading train data -------------------------"],"metadata":{"papermill":{"duration":17.742575,"end_time":"2023-10-30T00:45:36.347807","exception":false,"start_time":"2023-10-30T00:45:18.605232","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-13T01:40:09.433772Z","iopub.execute_input":"2023-12-13T01:40:09.433997Z","iopub.status.idle":"2023-12-13T01:40:25.497003Z","shell.execute_reply.started":"2023-12-13T01:40:09.433973Z","shell.execute_reply":"2023-12-13T01:40:25.495583Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"chPvI9VC78oh","executionInfo":{"status":"ok","timestamp":1702477292258,"user_tz":300,"elapsed":43315,"user":{"displayName":"Nima Shahbazi","userId":"15346950202304736225"}},"outputId":"496196c4-ff3c-49f8-8122-839d1fff5d75"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","The 'target' column has 88 NaN values.\n","converting prices columns to float32 values.\n"]}],"id":"chPvI9VC78oh"},{"cell_type":"code","source":["#@title functions\n","\n","def split_by_date(df, dates):\n","\n","    df_start, df_end = dates\n","    df = df[(df['date_id'] >= df_start) & (df['date_id'] <=df_end)].reset_index(drop=True)\n","\n","    return df\n","\n","\n","\n","def lag_function(df, columns_to_lag, numbers_of_days_to_lag):\n","\n","    df_indexed = df.set_index(['stock_id', 'seconds_in_bucket', 'date_id'])\n","\n","    for column_to_lag in columns_to_lag:\n","        for number_days_to_lag in numbers_of_days_to_lag:\n","            df_indexed[f'lag{number_days_to_lag}_{column_to_lag}'] = df_indexed.groupby(level=['stock_id', 'seconds_in_bucket'])[column_to_lag].shift(number_days_to_lag)\n","\n","    df_indexed.reset_index(inplace=True)\n","\n","    return df_indexed\n","\n","\n","\n","def create_diff_lagged_features_within_date_revised(df, columns_to_lag, numbers_of_lag):\n","    df_copy = df.copy()\n","\n","    # Store the new columns in a list\n","    new_columns = []\n","\n","    # Iterate through each specified lag\n","    for lag in numbers_of_lag:\n","        # Create lagged dataframe once per lag value\n","        lagged_df = df.groupby(['stock_id', 'date_id'])[columns_to_lag].shift(periods=lag)\n","\n","        # Iterate through each specified column\n","        for column in columns_to_lag:\n","            # Compute the new column\n","            new_col_name = f'{column}_diff_lag{lag}'\n","            new_column = df[column] - lagged_df[column]\n","\n","            # Store the new column in the list\n","            new_columns.append(new_column.rename(new_col_name))\n","\n","    # Concatenate the original dataframe with the new columns\n","    result_df = pd.concat([df_copy] + new_columns, axis=1)\n","\n","    return result_df\n","\n","\n","def create_features_to_start_optimized(df, features_list):\n","\n","    first_values_df = df.groupby(['stock_id', 'date_id'])[features_list].transform('first')\n","\n","    for feature in features_list:\n","        feature_to_start_col_name = f'{feature}_to_start'\n","        df[feature_to_start_col_name] = df[feature] - first_values_df[feature]\n","\n","    return df\n","\n","\n","def compute_imbalances(df_, columns, prefix = ''):\n","    \"\"\"Computes the differences and imbalances for pairs of columns and stores them in the DataFrame.\"\"\"\n","    df = df_.copy()\n","    for col1, col2 in combinations(columns, 2):\n","\n","        # Sort the columns lexicographically to ensure consistent ordering\n","        col1, col2 = sorted([col1, col2])\n","\n","        # Compute imbalance directly without creating a temporary difference column\n","        total = df[col1] + df[col2]\n","        imbalance_column_name = f'{col1}_{col2}_imb{prefix}'\n","\n","        # Ensure we don't divide by zero\n","        df[imbalance_column_name] = (df[col1] - df[col2]).divide(total, fill_value=np.nan)\n","\n","    return df\n","\n","def compute_percentage_difference(df, columns, prefix = ''):\n","\n","    df_copy = df.copy()\n","\n","    # Iterate over all combinations of two different price columns\n","    for col1, col2 in combinations(columns, 2):\n","        # Sort the columns lexicographically to ensure consistent ordering\n","        col1, col2 = sorted([col1, col2])\n","\n","        # Create a new column name based on the price columns\n","        new_col_name = f'pct_diff_{col1}_vs_{col2}_{prefix}'\n","\n","        # Compute the percentage difference\n","        df_copy[new_col_name] = (df_copy[col1] - df_copy[col2]) / df_copy[col2] * 100\n","\n","    return df_copy\n","\n","\n","\n","def create_deviation_within_seconds(df, num_features):\n","    groupby_cols = ['date_id', 'seconds_in_bucket']\n","    new_columns = {}  # Dictionary to hold new columns\n","\n","    for feature in num_features:\n","        grouped_median = df.groupby(groupby_cols)[feature].transform('median')\n","        deviation_col_name = f'deviation_from_median_{feature}'\n","        new_columns[deviation_col_name] = df[feature] - grouped_median\n","\n","    # Concatenate all new columns at once\n","    df = pd.concat([df, pd.DataFrame(new_columns)], axis=1)\n","    return df\n","\n","\n","def create_cumsum_features(df, columns_to_compute):\n","    df_copy = df.copy()\n","\n","    # Group by 'stock_id' and 'date_id' for cumulative sum calculation\n","    grouped = df_copy.groupby(['stock_id', 'date_id'])\n","\n","    # Calculate cumulative sum for each column within each group\n","    for column in columns_to_compute:\n","        cumsum_col_name = f'{column}_cumsum'\n","        df_copy[cumsum_col_name] = grouped[column].cumsum()\n","    return df_copy\n","\n","\n","def save_pickle(data, file_path):\n","\n","    # Create the directory if it doesn't exist\n","    directory = os.path.dirname(file_path)\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)\n","\n","    # Save the pickle file\n","    with open(file_path, 'wb') as file:\n","        pickle.dump(data, file)\n","\n","    print(f\"Data saved to {file_path}\")\n","    #example: save_pickle(all_data, 'k8/all_data.pkl')\n","\n","def load_pickle(file_path):\n","\n","    # Load and return the data from the pickle file\n","    if os.path.exists(file_path):\n","        with open(file_path, 'rb') as file:\n","            data = pickle.load(file)\n","        return data\n","    else:\n","        raise FileNotFoundError(f\"No such file: {file_path}\")\n","\n","    #example: loaded_data = load_pickle('k8/all_data.pkl')\n","\n"],"metadata":{"papermill":{"duration":0.048164,"end_time":"2023-10-30T00:45:36.402382","exception":false,"start_time":"2023-10-30T00:45:36.354218","status":"completed"},"tags":[],"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-12-13T01:40:25.499300Z","iopub.execute_input":"2023-12-13T01:40:25.499620Z","iopub.status.idle":"2023-12-13T01:40:25.520046Z","shell.execute_reply.started":"2023-12-13T01:40:25.499589Z","shell.execute_reply":"2023-12-13T01:40:25.519217Z"},"trusted":true,"cellView":"form","id":"Odw5lqHN78oi","executionInfo":{"status":"ok","timestamp":1702477292258,"user_tz":300,"elapsed":17,"user":{"displayName":"Nima Shahbazi","userId":"15346950202304736225"}}},"execution_count":4,"outputs":[],"id":"Odw5lqHN78oi"},{"cell_type":"code","source":["#@title global\n","#---------------------------------- Global based on stock_id --------------------------------------\n","\n","def aggregated_features_dic(df):\n","\n","\n","    global_feats = {}\n","\n","#     columns_to_aggregate= ['bid_size','ask_size']\n","#     groupby_cols=['stock_id']\n","\n","#     def q25(x):\n","#         return x.quantile(0.25)\n","\n","#     def q75(x):\n","#         return x.quantile(0.75)\n","\n","#     # Define the aggregations\n","#     aggregations = ['mean', 'median', 'std', 'min', 'max', q25, q75]\n","\n","#     # Prepare a dictionary to hold the aggregated Series\n","\n","\n","#     # Perform aggregation for each column and operation\n","#     for column in columns_to_aggregate:\n","#         for agg in aggregations:\n","#             # Define the aggregation function name\n","#             if callable(agg):\n","#                 func_name = agg.__name__\n","#             else:\n","#                 func_name = agg\n","\n","#             # Perform the aggregation\n","#             agg_series = df.groupby(groupby_cols)[column].agg(agg)\n","#             # Create a new feature name and add it to the dictionary\n","#             new_feature_name = f\"{func_name}_{column}\"\n","#             global_feats[new_feature_name] = agg_series\n","\n","    global_feats[\"median_size\"] = df.groupby(\"stock_id\")[\"bid_size\"].median() + df.groupby(\"stock_id\")[\"ask_size\"].median()\n","    global_feats[\"std_size\"] = df.groupby(\"stock_id\")[\"bid_size\"].std() + df.groupby(\"stock_id\")[\"ask_size\"].std()\n","    global_feats[\"ptp_size\"] = df.groupby(\"stock_id\")[\"bid_size\"].max() - df.groupby(\"stock_id\")[\"bid_size\"].min()\n","    global_feats[\"median_price\"] = df.groupby(\"stock_id\")[\"bid_price\"].median() + df.groupby(\"stock_id\")[\"ask_price\"].median()\n","    global_feats[\"std_price\"] = df.groupby(\"stock_id\")[\"bid_price\"].std() + df.groupby(\"stock_id\")[\"ask_price\"].std()\n","    global_feats[\"ptp_price\"] = df.groupby(\"stock_id\")[\"bid_price\"].max() - df.groupby(\"stock_id\")[\"ask_price\"].min()\n","\n","\n","    return global_feats\n","\n","aggregated_dic = aggregated_features_dic(train)\n","\n","def map_global(df,dict):\n","    df_ = df.copy()\n","    for key, value in dict.items():\n","        df_[f\"global_{key}\"] = df_[\"stock_id\"].map(value.to_dict())\n","\n","    return df_\n","#---------------------------------- Global based on stock_id --------------------------------------\n","\n"],"metadata":{"execution":{"iopub.status.busy":"2023-12-13T01:40:25.521007Z","iopub.execute_input":"2023-12-13T01:40:25.521234Z","iopub.status.idle":"2023-12-13T01:40:26.859774Z","shell.execute_reply.started":"2023-12-13T01:40:25.521210Z","shell.execute_reply":"2023-12-13T01:40:26.858491Z"},"trusted":true,"id":"qK_dtNGr78oj","executionInfo":{"status":"ok","timestamp":1702477293673,"user_tz":300,"elapsed":1418,"user":{"displayName":"Nima Shahbazi","userId":"15346950202304736225"}}},"execution_count":5,"outputs":[],"id":"qK_dtNGr78oj"},{"cell_type":"code","source":["#@title helper functions\n","\n","\n","def flatten_outliers_y_train(y_train, lower_quantile=0.01, upper_quantile=0.99):\n","\n","    lower_bound = np.quantile(y_train, lower_quantile)\n","    upper_bound = np.quantile(y_train, upper_quantile)\n","\n","    # Cap values below the lower bound and above the upper bound\n","    y_train_flattened = np.clip(y_train, lower_bound, upper_bound)\n","\n","    return y_train_flattened\n","\n","\n","def create_autocorrelation_features(df, columns, lags):\n","\n","    df_copy = df.copy()\n","\n","    for column in columns:\n","        for lag in lags:\n","            lagged_series = df_copy[column].shift(lag)\n","            df_copy[f'{column}_autocorr_lag{lag}'] = df_copy[column].corrwith(lagged_series)\n","\n","    return df_copy\n","\n","\n","def calculate_stat_lag(df, num_lags):\n","\n","    lags = [f'lag{i}_target' for i in range(1, num_lags + 1)]\n","\n","    df['target_mean'] = df[lags].mean(axis=1)\n","    df['target_std_dev'] = df[lags].std(axis=1)\n","    df['target_variance'] = df[lags].var(axis=1)\n","    df['target_median'] = df[lags].median(axis=1)\n","    df['target_range'] = df[lags].max(axis=1) - df[lags].min(axis=1)\n","\n","    return df\n","\n","\n","def calculate_stat(df, cols, prefix='prices'):\n","\n","    df[f'{prefix}_mean'] = df[cols].mean(axis=1)\n","    df[f'{prefix}_std_dev'] = df[cols].std(axis=1)\n","    df[f'{prefix}_variance'] = df[cols].var(axis=1)\n","    df[f'{prefix}_median'] = df[cols].median(axis=1)\n","    df[f'{prefix}_range'] = df[cols].max(axis=1) - df[cols].min(axis=1)\n","\n","    return df\n","\n"],"metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-12-13T01:40:26.860998Z","iopub.execute_input":"2023-12-13T01:40:26.861271Z","iopub.status.idle":"2023-12-13T01:40:26.870925Z","shell.execute_reply.started":"2023-12-13T01:40:26.861243Z","shell.execute_reply":"2023-12-13T01:40:26.870041Z"},"trusted":true,"id":"tR0l6h9k78ok","executionInfo":{"status":"ok","timestamp":1702477293673,"user_tz":300,"elapsed":5,"user":{"displayName":"Nima Shahbazi","userId":"15346950202304736225"}}},"execution_count":6,"outputs":[],"id":"tR0l6h9k78ok"},{"cell_type":"code","source":["#@title pipeline\n","\n","raw_cols          = ['imbalance_size','matched_size','bid_size','ask_size','reference_price','far_price','near_price','bid_price','ask_price','wap','imbalance_buy_sell_flag']\n","\n","columns_prices    = ['reference_price','far_price','near_price','bid_price','ask_price','wap']\n","columns_4prices   = ['reference_price','bid_price','ask_price','wap']\n","\n","columns_sizes     = ['imbalance_size','matched_size','bid_size','ask_size']\n","columns_flag      = ['imbalance_buy_sell_flag']\n","\n","\n","diff_lags           = [1, 2, 3, 6, 12, 18, 24]\n","diff_lags_extra     = [30, 36, 42, 48]\n","\n","num_of_target_lags  = 3\n","target_lags         = list(range(1,num_of_target_lags+1))\n","\n","\n","def feature_engineering(df):\n","\n","    df = df.copy()\n","\n","    df['spread_eng']                            = df['ask_price'] - df['bid_price']\n","    df['volume_eng']                            = df['bid_size'] + df['ask_size']\n","    df['volumne_imbalance_eng']                 = df['bid_size'] - df['ask_size']\n","\n","    df['imbalance_ratio']                       = df['imbalance_size'] / df['matched_size']  #(RM) Good\n","\n","    df['price_spread_near_far']                 = df['near_price'] - df['far_price']   #RM (debatable)\n","    df['price_wap_difference_eng']              = df['reference_price'] - df['wap']\n","\n","    df['weighted_imbalance_eng']                = df['imbalance_size'] * df['imbalance_buy_sell_flag'] #very important\n","\n","\n","    df['bid_ask_ratio']                         = df['bid_size'] / df['ask_size'] #(RM) neutral\n","    df['imbalance_to_bid_ratio_eng']            = df['imbalance_size'] / df['bid_size']\n","    df['imbalance_to_ask_ratio_eng']            = df['imbalance_size'] / df['ask_size']\n","    df['matched_size_to_total_size_ratio_eng']  = df['matched_size'] / (df['bid_size'] + df['ask_size'])\n","\n","\n","    return df\n","\n","\n","\n","def feature_pipeline(df):\n","\n","    if df.empty:\n","        return pd.DataFrame()\n","\n","    #--------------- connected ------------\n","#     df = feature_engineering(df)\n","    df = compute_imbalances(df, columns_sizes,prefix='_sz_')\n","    df = compute_imbalances(df, columns_prices,prefix = '_pr_')\n","\n","\n","\n","    eng_features       = [feature for feature in df.columns if \"_eng\" in feature]\n","    imb_features_all   = [feature for feature in df.columns if \"_imb_\" in feature]\n","    imb_features_price = [feature for feature in df.columns if \"_pr_\" in feature]\n","    imb_features_size  = [feature for feature in df.columns if \"_sz_\" in feature]\n","\n","    #--------------- connected ------------\n","\n","\n","#     diff_lag_cols = raw_cols + eng_features\n","#     print(f\"diff lagging {len(diff_lag_cols)} columns for {len(diff_lags)} lags.\")\n","#     df = create_diff_lagged_features_within_date_revised(df,diff_lag_cols,diff_lags)\n","\n","#     cumsum_columns = columns_sizes + imb_features_size + eng_features\n","#     print(f\"cumsum for {len(cumsum_columns)} cols.\")\n","#     df = create_cumsum_features(df, cumsum_columns)\n","\n","    deviation_cols = raw_cols + eng_features + imb_features_size  + imb_features_price\n","    print(f\"deviation {len(deviation_cols)} columns within seconds.\")\n","    df = create_deviation_within_seconds(df,deviation_cols)\n","\n","    print(f\"lagging target column for {len(target_lags)} lags.\")\n","    df = lag_function(df, ['target'], target_lags)\n","\n","    df = map_global(df,aggregated_dic)\n","\n","    # df = calculate_stat_lag(df, num_lags=num_of_target_lags)\n","\n","    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n","\n","    print(\"Done...\")\n","\n","    return df"],"metadata":{"papermill":{"duration":0.020161,"end_time":"2023-10-30T00:45:36.456310","exception":false,"start_time":"2023-10-30T00:45:36.436149","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-13T01:40:26.871903Z","iopub.execute_input":"2023-12-13T01:40:26.872145Z","iopub.status.idle":"2023-12-13T01:40:26.886745Z","shell.execute_reply.started":"2023-12-13T01:40:26.872121Z","shell.execute_reply":"2023-12-13T01:40:26.886007Z"},"trusted":true,"id":"0rJWHorY78ol","executionInfo":{"status":"ok","timestamp":1702477293673,"user_tz":300,"elapsed":4,"user":{"displayName":"Nima Shahbazi","userId":"15346950202304736225"}}},"execution_count":7,"outputs":[],"id":"0rJWHorY78ol"},{"cell_type":"code","source":["#@title NN second pass\n","#------------------------------------------------- NN Second Pass Functions -----------------------------------------------\n","\n","def make_predictions(models, X_test,model = 'nn'):\n","    if model == 'nn':\n","        all_predictions = [model.predict(X_test, batch_size=16384) for model in models]\n","    if model == 'lgb' or model == 'xgb' or model == 'cat':\n","        all_predictions = [model.predict(X_test) for model in models]\n","    prediction = np.mean(all_predictions, axis=0)\n","    return prediction\n","\n","def set_all_seeds(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    tf.random.set_seed(seed)\n","\n","class BestScoresCallback(Callback):\n","    def __init__(self):\n","        super().__init__()\n","        self.best_train_loss = float('inf')\n","        self.best_val_loss = float('inf')\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        train_loss = logs.get('loss', float('inf'))\n","        val_loss = logs.get('val_loss', float('inf'))\n","\n","        if train_loss < self.best_train_loss:\n","            self.best_train_loss = train_loss\n","        if val_loss < self.best_val_loss:\n","            self.best_val_loss = val_loss\n","\n","    def on_train_end(self, logs=None):\n","        print(f\"Best training loss: {self.best_train_loss}, Best validation loss: {self.best_val_loss}\")\n","\n","\n","def second_pass_for_nn(df, numerical_features, categorical_features, is_inference=False):\n","    # Check if the DataFrame is empty\n","    global scaler,medians\n","\n","    if df.empty:\n","        return None, None\n","\n","    # Work on a copy of the DataFrame to avoid changing the original df\n","    df_copy = df.copy()\n","\n","\n","    # Standard scaling for numerical features\n","    if is_inference:\n","        df_copy.fillna(medians, inplace=True)\n","        df_copy[numerical_features] = scaler.transform(df_copy[numerical_features])\n","\n","    # Preprocess Data\n","    df_copy['seconds_in_bucket'] = df_copy['seconds_in_bucket'] / 10\n","    df_copy['imbalance_buy_sell_flag'] += 1\n","\n","    # Create input vector\n","    X_cat = [df_copy[cat].values.reshape(-1, 1) for cat in categorical_features]\n","    X_num = df_copy[numerical_features].values\n","\n","\n","    y = df_copy['target'].values\n","\n","    return [X_num] + X_cat, y\n","\n","#------------------------------------------------- NN Second Pass Functions -----------------------------------------------"],"metadata":{"papermill":{"duration":0.03576,"end_time":"2023-10-30T00:45:36.517275","exception":false,"start_time":"2023-10-30T00:45:36.481515","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-13T01:40:26.887952Z","iopub.execute_input":"2023-12-13T01:40:26.888217Z","iopub.status.idle":"2023-12-13T01:40:26.905411Z","shell.execute_reply.started":"2023-12-13T01:40:26.888190Z","shell.execute_reply":"2023-12-13T01:40:26.904630Z"},"trusted":true,"cellView":"form","id":"inHoh5ka78ol","executionInfo":{"status":"ok","timestamp":1702477293673,"user_tz":300,"elapsed":4,"user":{"displayName":"Nima Shahbazi","userId":"15346950202304736225"}}},"execution_count":8,"outputs":[],"id":"inHoh5ka78ol"},{"cell_type":"code","source":["#@title NN model\n","\n","def create_model(categorical_features,numerical_features,initial_learning_rate=0.001):\n","\n","    inputs = []\n","    embeddings = []\n","\n","    categorical_uniques = {}\n","    categorical_uniques['seconds_in_bucket'] = 55\n","\n","    embedding_dim = {}\n","    embedding_dim['seconds_in_bucket'] = 10\n","\n","\n","    input_num = Input(shape=(len(numerical_features),), name=\"numerical_input\")\n","    inputs.append(input_num)\n","\n","    for cat_feature in categorical_features:\n","        vocab_size = categorical_uniques[cat_feature]\n","        input_cat = Input(shape=(1,), name=f\"input_{cat_feature}\")\n","        embedding = Embedding(vocab_size, embedding_dim[cat_feature], input_length=1, name=f\"embedding_{cat_feature}\")(input_cat)\n","        flattened_embedding = Flatten(name=f\"flatten_{cat_feature}\")(embedding)\n","        inputs.append(input_cat)\n","        embeddings.append(flattened_embedding)\n","\n","    dense_output = concatenate(embeddings + [input_num])\n","\n","    dense_sizes = [512, 256, 128, 64, 32]\n","\n","    for size in dense_sizes:\n","        dense_output = Dense(size, activation='swish')(dense_output)\n","        dense_output = BatchNormalization()(dense_output)\n","        dense_output = Dropout(0.4)(dense_output)\n","\n","    output = Dense(1)(dense_output)\n","\n","\n","    lr_schedule = ExponentialDecay(\n","    initial_learning_rate=initial_learning_rate,\n","    decay_steps=3000,\n","    decay_rate=0.5,\n","    staircase=True)\n","\n","    model = Model(inputs, output)\n","    optimizer = Adam(learning_rate=lr_schedule)\n","\n","    model.compile(optimizer=optimizer, loss = \"mean_absolute_error\")\n","\n","    return model"],"metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-12-13T01:40:26.906446Z","iopub.execute_input":"2023-12-13T01:40:26.906717Z","iopub.status.idle":"2023-12-13T01:40:26.921459Z","shell.execute_reply.started":"2023-12-13T01:40:26.906690Z","shell.execute_reply":"2023-12-13T01:40:26.920699Z"},"trusted":true,"cellView":"form","id":"pC_U1ypM78om","executionInfo":{"status":"ok","timestamp":1702477293673,"user_tz":300,"elapsed":4,"user":{"displayName":"Nima Shahbazi","userId":"15346950202304736225"}}},"execution_count":9,"outputs":[],"id":"pC_U1ypM78om"},{"cell_type":"code","source":["#@title RNN second pass\n","\n","def precompute_sequences(stock_data, window_size, rnn_numerical_features, rnn_categorical_features):\n","    # Convert DataFrame columns to NumPy arrays\n","    stock_data_num = stock_data[rnn_numerical_features].values\n","    stock_data_cat = stock_data[rnn_categorical_features].values\n","\n","    # Pre-compute all sequences\n","    all_sequences_num = [stock_data_num[max(0, i - window_size + 1):i + 1] for i in range(len(stock_data))]\n","    all_sequences_cat = [stock_data_cat[max(0, i - window_size + 1):i + 1] for i in range(len(stock_data))]\n","\n","    # Add padding if necessary\n","    padded_sequences_num = [np.pad(seq, ((window_size - len(seq), 0), (0, 0)), 'constant') for seq in all_sequences_num]\n","    padded_sequences_cat = [np.pad(seq, ((window_size - len(seq), 0), (0, 0)), 'constant') for seq in all_sequences_cat]\n","\n","    # Combine numerical and categorical features\n","    combined_sequences = np.array([np.concatenate([num, cat], axis=-1)\n","                                   for num, cat in zip(padded_sequences_num, padded_sequences_cat)])\n","\n","    # Extract targets\n","    targets = stock_data['target'].values\n","\n","    return combined_sequences, targets\n","\n","def get_sequence(precomputed_data, time_step):\n","    combined_sequences, targets = precomputed_data\n","    return combined_sequences[time_step], targets[time_step]\n","\n","\n","\n","def create_batches(data, window_size, rnn_numerical_features, rnn_categorical_features, max_time_steps=55):\n","\n","    grouped = data.groupby(['stock_id', 'date_id'])\n","    all_batches = []\n","    all_targets = []\n","\n","    for _, group in tqdm(grouped, desc=\"Processing groups\"):\n","        # Precompute sequences for the current group\n","        precomputed_data = precompute_sequences(group, window_size, rnn_numerical_features, rnn_categorical_features)\n","\n","        # Initialize containers for group sequences and targets\n","        group_sequences = []\n","        group_targets = []\n","\n","        # Iterate over the time steps and retrieve precomputed sequences\n","        for time_step in range(max_time_steps):\n","            sequence, target = get_sequence(precomputed_data, time_step)\n","            if sequence.size > 0:\n","                group_sequences.append(sequence)\n","                group_targets.append(target)\n","\n","        # Extend the main batches with the group's sequences and targets\n","        all_batches.extend(group_sequences)\n","        all_targets.extend(group_targets)\n","\n","    return all_batches, all_targets\n","\n","\n","\n","\n","def compute_last_sequence(group, window_size, rnn_numerical_features, rnn_categorical_features):\n","    # Convert DataFrame columns to NumPy arrays\n","    stock_data_num      = group[rnn_numerical_features].values\n","    stock_data_cat      = group[rnn_categorical_features].values\n","    stock_data_target   = group['target'].values\n","\n","    # Find the index of the target second\n","    target_index = len(group) - 1\n","\n","    # Extract the sequence for the target index\n","    sequence_num = stock_data_num[max(0, target_index - window_size + 1):target_index + 1]\n","    sequence_cat = stock_data_cat[max(0, target_index - window_size + 1):target_index + 1]\n","\n","    # Add padding if necessary\n","    padded_sequence_num = np.pad(sequence_num, ((window_size - len(sequence_num), 0), (0, 0)), 'constant')\n","    padded_sequence_cat = np.pad(sequence_cat, ((window_size - len(sequence_cat), 0), (0, 0)), 'constant')\n","\n","    # Combine numerical and categorical features\n","    combined_sequence = np.concatenate([padded_sequence_num, padded_sequence_cat], axis=-1)\n","\n","    # Extract target\n","    target = stock_data_target[-1]\n","\n","    return combined_sequence, target\n","\n","\n","def create_last_batches(data, window_size, rnn_numerical_features, rnn_categorical_features):\n","\n","    grouped = data.groupby(['stock_id'])\n","    all_batches = []\n","    all_targets = []\n","\n","    for _, group in grouped:\n","        # Compute the sequence for the last data point in the current group\n","        last_sequence, last_target = compute_last_sequence(group, window_size, rnn_numerical_features, rnn_categorical_features)\n","\n","        # Check if the sequence is valid (i.e., not empty)\n","        if last_sequence.size > 0:\n","            all_batches.append(last_sequence)\n","            all_targets.append(last_target)\n","\n","    return all_batches, all_targets\n","\n","\n","\n","\n","\n","def second_pass_for_rnn(df, rnn_numerical_features, rnn_categorical_features, window_size, is_inference=False):\n","    # Check if the DataFrame is empty\n","    global rnn_scaler,rnn_medians\n","\n","    if df.empty:\n","        return None, None\n","\n","    # Work on a copy of the DataFrame to avoid changing the original df\n","    df_copy = df.copy()\n","\n","    # Standard scaling for numerical features\n","    if is_inference:\n","        df_copy.fillna(rnn_medians, inplace=True)\n","        df_copy[rnn_numerical_features] = rnn_scaler.transform(df_copy[rnn_numerical_features])\n","\n","    # Preprocess Data\n","    df_copy['seconds_in_bucket'] = df_copy['seconds_in_bucket'] / 10\n","    df_copy['imbalance_buy_sell_flag'] += 1\n","\n","    if is_inference:\n","        df_copy_batches, df_copy_targets = create_last_batches(df_copy, window_size, rnn_numerical_features, rnn_categorical_features)\n","    else:\n","        df_copy_batches, df_copy_targets = create_batches(df_copy, window_size, rnn_numerical_features, rnn_categorical_features)\n","\n","    df_copy_batches = np.array(df_copy_batches)\n","    df_copy_targets = np.array(df_copy_targets)\n","\n","\n","    return df_copy_batches, df_copy_targets\n","\n","\n","def online_pass_for_rnn(df, rnn_numerical_features, rnn_categorical_features, window_size):\n","    # Check if the DataFrame is empty\n","    global rnn_scaler,rnn_medians\n","\n","    if df.empty:\n","        return None, None\n","\n","    # Work on a copy of the DataFrame to avoid changing the original df\n","    df_copy = df.copy()\n","\n","    # Standard scaling for numerical features\n","    df_copy.fillna(rnn_medians, inplace=True)\n","    df_copy[rnn_numerical_features] = rnn_scaler.transform(df_copy[rnn_numerical_features])\n","\n","    # Preprocess Data\n","    df_copy['seconds_in_bucket'] = df_copy['seconds_in_bucket'] / 10\n","    df_copy['imbalance_buy_sell_flag'] += 1\n","\n","\n","    grouped = df_copy.groupby(['stock_id'])\n","    all_batches = []\n","    all_targets = []\n","\n","    for _, group in tqdm(grouped, desc=\"Processing groups\"):\n","        # Precompute sequences for the current group\n","        precomputed_data = precompute_sequences(group, window_size, rnn_numerical_features, rnn_categorical_features)\n","\n","        # Initialize containers for group sequences and targets\n","        group_sequences = []\n","        group_targets = []\n","\n","        # Iterate over the time steps and retrieve precomputed sequences\n","        for time_step in range(55):\n","            sequence, target = get_sequence(precomputed_data, time_step)\n","            if sequence.size > 0:\n","                group_sequences.append(sequence)\n","                group_targets.append(target)\n","\n","        # Extend the main batches with the group's sequences and targets\n","        all_batches.extend(group_sequences)\n","        all_targets.extend(group_targets)\n","\n","    df_batches = np.array(all_batches)\n","    df_targets = np.array(all_targets)\n","\n","\n","    return df_batches, df_targets\n","\n"],"metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-12-13T01:40:26.924574Z","iopub.execute_input":"2023-12-13T01:40:26.924853Z","iopub.status.idle":"2023-12-13T01:40:26.946571Z","shell.execute_reply.started":"2023-12-13T01:40:26.924825Z","shell.execute_reply":"2023-12-13T01:40:26.945787Z"},"trusted":true,"cellView":"form","id":"kl7j4Jbr78om","executionInfo":{"status":"ok","timestamp":1702477293673,"user_tz":300,"elapsed":3,"user":{"displayName":"Nima Shahbazi","userId":"15346950202304736225"}}},"execution_count":10,"outputs":[],"id":"kl7j4Jbr78om"},{"cell_type":"code","source":["#@title RNN model\n","\n","def apply_conv_layers(input_layer, kernel_sizes, filters=16, do_ratio=0.5):\n","    conv_outputs = []\n","\n","    for kernel_size in kernel_sizes:\n","        conv_layer = Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', padding='same')(input_layer)\n","        conv_layer = BatchNormalization()(conv_layer)\n","        conv_layer = Dropout(do_ratio)(conv_layer)\n","\n","        shortcut = conv_layer\n","\n","        conv_layer = Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(conv_layer)\n","        conv_layer = BatchNormalization()(conv_layer)\n","        conv_layer = Activation('relu')(conv_layer)\n","\n","        # Add the output of the first Conv1D layer\n","        conv_layer = Add()([conv_layer, shortcut])\n","        conv_outputs.append(conv_layer)\n","\n","\n","    concatenated_conv = Concatenate(axis=-1)(conv_outputs)\n","    flattened_conv_output = Flatten()(concatenated_conv)\n","\n","    return flattened_conv_output\n","\n","def create_rnn_model_with_residual(window_size, numerical_features, initial_learning_rate=0.001):\n","\n","    categorical_features = 'seconds_in_bucket'\n","    categorical_uniques  = { 'seconds_in_bucket' : 55}\n","    embedding_dim        = {'seconds_in_bucket' : 10}\n","\n","    input_layer = Input(shape=(window_size, len(numerical_features) + 1), name=\"combined_input\")\n","\n","    # Split the input into numerical and categorical parts\n","    numerical_input = Lambda(lambda x: x[:, :, :-1], name=\"numerical_part\")(input_layer)\n","    categorical_input = Lambda(lambda x: x[:, :, -1:], name=\"categorical_part\")(input_layer)\n","\n","    first_numerical = Lambda(lambda x: x[:, 0])(numerical_input)\n","\n","\n","    # diffrentiate layers\n","    def create_difference_layer(lag):\n","        return Lambda(lambda x: x[:, lag:, :] - x[:, :-lag, :], name=f\"difference_layer_lag{lag}\")\n","\n","    difference_layers = []\n","    for lag in range(1, window_size):\n","        diff_layer = create_difference_layer(lag)(numerical_input)\n","        padding = ZeroPadding1D(padding=(lag, 0))(diff_layer)  # Add padding to the beginning of the sequence\n","        difference_layers.append(padding)\n","    combined_diff_layer = Concatenate(name=\"combined_difference_layer\")(difference_layers)\n","\n","\n","    # Embedding for categorical part\n","    vocab_size, embedding_dim = categorical_uniques[categorical_features], embedding_dim[categorical_features]\n","    embedding = Embedding(vocab_size, embedding_dim, input_length=window_size)(categorical_input)\n","    embedding = Reshape((window_size, -1))(embedding)\n","\n","    first_embedding = Lambda(lambda x: x[:, 0])(embedding)\n","\n","    # Concatenate numerical input and embedding\n","    # conv_input = concatenate([enhanced_numerical_input, embedding], axis=-1)\n","\n","    kernel_sizes = [2,3]\n","    do_ratio = 0.4\n","\n","    flattened_conv_output = apply_conv_layers(numerical_input, kernel_sizes, do_ratio=do_ratio)\n","    flattened_conv_output_cat = apply_conv_layers(embedding, kernel_sizes, do_ratio=do_ratio)\n","    flattened_conv_output_diff = apply_conv_layers(combined_diff_layer, kernel_sizes, do_ratio=do_ratio)\n","\n","\n","    dense_output = Concatenate(axis=-1)([flattened_conv_output,flattened_conv_output_cat,flattened_conv_output_diff, Reshape((-1,))(combined_diff_layer),first_numerical,first_embedding])\n","\n","    dense_sizes = [512, 256, 128, 64, 32]\n","    do_ratio = 0.5\n","    for size in dense_sizes:\n","        dense_output = Dense(size, activation='swish')(dense_output)\n","        dense_output = BatchNormalization()(dense_output)\n","        dense_output = Dropout(do_ratio)(dense_output)\n","\n","    # Output layer\n","    output = Dense(1, name='output_layer')(dense_output)\n","\n","    # Learning rate schedule\n","    lr_schedule = ExponentialDecay(\n","        initial_learning_rate=initial_learning_rate,\n","        decay_steps=10000,\n","        decay_rate=0.7,\n","        staircase=True)\n","\n","    # Create and compile the model\n","    model = Model(inputs=input_layer, outputs=output)\n","    optimizer = Adam(learning_rate=lr_schedule)\n","\n","    model.compile(optimizer=optimizer, loss=\"mean_absolute_error\")\n","\n","    return model"],"metadata":{"execution":{"iopub.status.busy":"2023-12-13T01:40:26.947663Z","iopub.execute_input":"2023-12-13T01:40:26.947935Z","iopub.status.idle":"2023-12-13T01:40:26.964527Z","shell.execute_reply.started":"2023-12-13T01:40:26.947908Z","shell.execute_reply":"2023-12-13T01:40:26.963777Z"},"trusted":true,"id":"AVwvP9HX78on","executionInfo":{"status":"ok","timestamp":1702477293673,"user_tz":300,"elapsed":2,"user":{"displayName":"Nima Shahbazi","userId":"15346950202304736225"}}},"execution_count":11,"outputs":[],"id":"AVwvP9HX78on"},{"cell_type":"code","source":["#@title runnig pipeline\n","\n","excluded_columns = ['row_id', 'date_id', 'time_id', 'target','stock_return']\n","\n","if run_pipeline:\n","\n","    train_eng = feature_pipeline(train)\n","\n","\n","    if is_nn:\n","        excluded_columns = excluded_columns + ['stock_id']\n","\n","        features = [col for col in train_eng.columns if col not in excluded_columns]\n","        categorical_features =  ['seconds_in_bucket']\n","        numerical_features = [feat for feat in features if feat not in categorical_features]\n","        print(\"we have {} numerical and {} categorical\".format(len(numerical_features),len(categorical_features)))\n","\n","\n","\n","        scaler = StandardScaler()\n","        medians = train_eng.median()\n","\n","        train_eng.fillna(medians, inplace=True)\n","        train_eng[numerical_features] = scaler.fit_transform(train_eng[numerical_features])\n","\n","\n","        all_data = {\n","            \"scaler\": scaler,\n","            \"medians\": medians,\n","            \"categorical_features\": categorical_features,\n","            \"numerical_features\": numerical_features\n","        }\n","\n","        save_pickle(all_data, f'{models_path}all_data.pkl')\n","        print(\"Pipline Done!\")\n","\n","    if is_rnn:\n","        excluded_columns = excluded_columns + ['stock_id']\n","\n","        features = [col for col in train_eng.columns if col not in excluded_columns]\n","        rnn_categorical_features =  ['seconds_in_bucket']\n","        rnn_numerical_features = [feat for feat in features if feat not in rnn_categorical_features]\n","        print(\"we have {} numerical and {} categorical\".format(len(rnn_numerical_features),len(rnn_categorical_features)))\n","\n","\n","        rnn_scaler = StandardScaler()\n","        rnn_medians = train_eng.median()\n","\n","        train_eng.fillna(rnn_medians, inplace=True)\n","        train_eng[rnn_numerical_features] = rnn_scaler.fit_transform(train_eng[rnn_numerical_features])\n","\n","\n","        rnn_all_data = {\n","            \"rnn_scaler\": rnn_scaler,\n","            \"rnn_medians\": rnn_medians,\n","            \"rnn_categorical_features\": rnn_categorical_features,\n","            \"rnn_numerical_features\": rnn_numerical_features\n","        }\n","\n","        save_pickle(rnn_all_data, f'{models_path}rnn_all_data.pkl')\n","        print(\"Pipline Done!\")\n","\n","    if is_lgb:\n","        lgb_features = [col for col in train_eng.columns if col not in excluded_columns]\n","        categorical_features = ['seconds_in_bucket']\n","\n","        print(\"we have {} lgb features\".format(len(lgb_features)))\n","\n","    train_data       = split_by_date(train_eng, dates_train)\n","    test_data        = split_by_date(train_eng, dates_test)\n","    print(\"number of dates in train = {} , number of dates in test {}\".format (train_data['date_id'].nunique(),test_data['date_id'].nunique()))\n","\n","    cleaning = False\n","    if cleaning:\n","        import gc\n","        #del train\n","        del train_eng\n","        gc.collect()\n","\n","\n"],"metadata":{"papermill":{"duration":0.404747,"end_time":"2023-10-30T00:45:36.928334","exception":false,"start_time":"2023-10-30T00:45:36.523587","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-13T01:40:26.965564Z","iopub.execute_input":"2023-12-13T01:40:26.965830Z","iopub.status.idle":"2023-12-13T01:41:23.424709Z","shell.execute_reply.started":"2023-12-13T01:40:26.965804Z","shell.execute_reply":"2023-12-13T01:41:23.423604Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"ff1mJMFj78on","executionInfo":{"status":"ok","timestamp":1702477352102,"user_tz":300,"elapsed":58431,"user":{"displayName":"Nima Shahbazi","userId":"15346950202304736225"}},"outputId":"65571b9f-2db5-4e47-9716-1d468376ed09"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["deviation 32 columns within seconds.\n","lagging target column for 3 lags.\n","Done...\n","we have 73 numerical and 1 categorical\n","Data saved to /content/drive/My Drive/optiver/optiver-258-rnn-inference-74/rnn_all_data.pkl\n","Pipline Done!\n","number of dates in train = 481 , number of dates in test 0\n"]}],"id":"ff1mJMFj78on"},{"cell_type":"code","source":["#@title TPU\n","try:\n","    # Create a TPUClusterResolver\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","    # Connect to the TPU cluster\n","    tf.config.experimental_connect_to_cluster(tpu)\n","    # Initialize the TPU system\n","    tf.tpu.experimental.initialize_tpu_system(tpu)\n","    # Create a TPUStrategy for distributed training\n","    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","except ValueError:\n","    tpu_strategy = None  # No TPU found\n"],"metadata":{"papermill":{"duration":0.01349,"end_time":"2023-10-30T00:45:36.967612","exception":false,"start_time":"2023-10-30T00:45:36.954122","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-13T01:41:23.426037Z","iopub.execute_input":"2023-12-13T01:41:23.426323Z","iopub.status.idle":"2023-12-13T01:41:31.193202Z","shell.execute_reply.started":"2023-12-13T01:41:23.426296Z","shell.execute_reply":"2023-12-13T01:41:31.191937Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true,"cellView":"form","id":"HYfNqtG078oo","executionInfo":{"status":"ok","timestamp":1702477352103,"user_tz":300,"elapsed":18,"user":{"displayName":"Nima Shahbazi","userId":"15346950202304736225"}}},"execution_count":13,"outputs":[],"id":"HYfNqtG078oo"},{"cell_type":"code","source":["#@title prepare train models\n","if train_models:\n","\n","    if test_data.empty:\n","        print(\"--------------test data is empty so adjusting the last date for test-----------------\")\n","        test_data = train_data.query(\"date_id == 480\").copy()\n","\n","    if is_lgb:\n","\n","\n","        X_train, y_train = train_data[lgb_features], train_data['target']\n","        X_test, y_test = test_data[lgb_features],test_data['target']\n","        train_set = lgb.Dataset(X_train, label=y_train,categorical_feature=categorical_features,free_raw_data=False)\n","        test_set = lgb.Dataset(X_test, label=y_test,categorical_feature=categorical_features,free_raw_data=False)\n","\n","    if is_nn:\n","\n","        X_train, y_train    = second_pass_for_nn(train_data,numerical_features,categorical_features)\n","        X_test, y_test      = second_pass_for_nn(test_data,numerical_features,categorical_features)\n","\n","    if is_rnn:\n","\n","        train_batches, train_targets = second_pass_for_rnn(train_data, rnn_numerical_features, rnn_categorical_features, window_size)\n","        test_batches, test_targets  = second_pass_for_rnn(test_data, rnn_numerical_features, rnn_categorical_features, window_size)\n","        print(f\"train batches shape:{train_batches.shape}\")\n"],"metadata":{"execution":{"iopub.status.busy":"2023-12-13T01:41:31.194447Z","iopub.execute_input":"2023-12-13T01:41:31.194755Z","iopub.status.idle":"2023-12-13T01:49:44.690860Z","shell.execute_reply.started":"2023-12-13T01:41:31.194700Z","shell.execute_reply":"2023-12-13T01:49:44.689985Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"rMVaGjaM78oo","executionInfo":{"status":"ok","timestamp":1702477957474,"user_tz":300,"elapsed":605375,"user":{"displayName":"Nima Shahbazi","userId":"15346950202304736225"}},"outputId":"05be4616-0c06-4d62-8d0c-0be01dd21208"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["--------------test data is empty so adjusting the last date for test-----------------\n"]},{"output_type":"stream","name":"stderr","text":["Processing groups: 100%|██████████| 95236/95236 [09:47<00:00, 162.03it/s]\n","Processing groups: 100%|██████████| 200/200 [00:01<00:00, 163.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train batches shape:(5237980, 3, 74)\n"]}],"id":"rMVaGjaM78oo"},{"cell_type":"code","source":["#@title training models\n","# train_data = flatten_outliers(train_data, 'target', lower_quantile=0.01, upper_quantile=0.99)\n","if train_models:\n","\n","        directory = os.path.dirname(models_path)\n","        if not os.path.exists(directory):\n","            os.makedirs(directory)\n","\n","        if is_lgb:\n","            lgb_models = []\n","            for i in range(num_models['lgb']):\n","                rnd_state=42+i\n","                print(f\"Training model {i+1} out of {num_models['lgb']} with seed {rnd_state}\")\n","                print(\"---------------------------------------\")\n","\n","                lgb_params['random_state'] = rnd_state\n","                lgb_model = lgb.train(lgb_params, train_set,init_model=None, valid_sets=[train_set, test_set],verbose_eval=50, early_stopping_rounds=250)\n","                lgb_model.save_model(f'{models_path}model_lgb_{i}.txt')\n","                lgb_models.append(lgb_model)\n","\n","                if dates_train[1]!=480:\n","                    pred = lgb_model.predict(X_test)\n","                    mae = mean_absolute_error(test_data['target'] , pred)\n","                    print(f\"Mean Absolute Error on test data: {mae:.5f}\")\n","\n","            if dates_train[1]!=480:\n","                predictions =  make_predictions(lgb_models, test_data[lgb_features],model = 'lgb')\n","                print(f\"LGB Ensemble Mean Absolute Error: {mean_absolute_error(test_data['target'], predictions):.5f}\")\n","\n","        if is_nn:\n","\n","            callbacks = [BestScoresCallback()]  # Always include BestScoresCallback\n","            if dates_train[1] != 480:\n","                early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False)\n","                callbacks.append(early_stopping)\n","\n","            if tpu_strategy:\n","                with tpu_strategy.scope():\n","\n","                    nn_models = []\n","                    for i in range(num_models['nn']):\n","                        print(f\"Training nn model {i+1} out of {num_models['nn']} with seed {42+i}\")\n","                        print(\"---------------------------------------\")\n","                        set_all_seeds(42+i)\n","\n","                        nn_model = create_model(categorical_features, numerical_features, initial_learning_rate=nn_lr)\n","                        history = nn_model.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=nn_ep, batch_size=nn_bs, callbacks=callbacks)\n","\n","                        print(\"---------------------------------------\")\n","                        nn_model.save(f'{models_path}swish_model_seed_{i}.h5')\n","                        nn_models.append(nn_model)\n","\n","                    predictions =  make_predictions(nn_models, X_test,model ='nn')\n","\n","                    print(f\"Ensemble Mean Absolute Error: {mean_absolute_error(y_test, predictions):.4f}\")\n","\n","        if is_rnn:\n","\n","            callbacks = [BestScoresCallback()]  # Always include BestScoresCallback\n","            if dates_train[1] != 480:\n","                early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False)\n","                callbacks.append(early_stopping)\n","\n","            if True or tpu_strategy:\n","                # with tpu_strategy.scope():\n","\n","                    rnn_models = []\n","                    for i in range(num_models['rnn']):\n","\n","                        print(f\"Training rnn model {i+1} out of {num_models['rnn']} with seed {42+i}\")\n","                        print(\"---------------------------------------\")\n","                        set_all_seeds(42+i)\n","\n","                        rnn_model = create_rnn_model_with_residual(window_size, rnn_numerical_features, initial_learning_rate=rnn_lr)\n","                        history = rnn_model.fit(train_batches, train_targets, validation_data=(test_batches, test_targets), epochs=rnn_ep, batch_size=rnn_bs, callbacks=callbacks)\n","                        print(\"---------------------------------------\")\n","                        rnn_model.save(f'{models_path}rnn_model_seed_{i}.h5')\n","                        rnn_models.append(rnn_model)\n","\n","                    predictions =  make_predictions(rnn_models, test_batches,model = 'nn')\n","                    print(f\"Ensemble Mean Absolute Error: {mean_absolute_error(test_targets, predictions):.4f}\")\n"],"metadata":{"papermill":{"duration":0.017103,"end_time":"2023-10-30T00:45:36.990986","exception":false,"start_time":"2023-10-30T00:45:36.973883","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-13T01:49:44.692527Z","iopub.execute_input":"2023-12-13T01:49:44.692840Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"quC3VRDB78oo","executionInfo":{"status":"ok","timestamp":1702482805490,"user_tz":300,"elapsed":4848018,"user":{"displayName":"Nima Shahbazi","userId":"15346950202304736225"}},"outputId":"166e8af9-b9b8-40b1-8739-bd388247ad87"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Training rnn model 1 out of 2 with seed 42\n","---------------------------------------\n","Epoch 1/100\n","1279/1279 [==============================] - 48s 20ms/step - loss: 6.3174 - val_loss: 4.8270\n","Epoch 2/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2764 - val_loss: 4.8213\n","Epoch 3/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2642 - val_loss: 4.8123\n","Epoch 4/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2564 - val_loss: 4.8046\n","Epoch 5/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2515 - val_loss: 4.8034\n","Epoch 6/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2460 - val_loss: 4.8127\n","Epoch 7/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2427 - val_loss: 4.8118\n","Epoch 8/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2386 - val_loss: 4.8082\n","Epoch 9/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2332 - val_loss: 4.8031\n","Epoch 10/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2311 - val_loss: 4.8011\n","Epoch 11/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2281 - val_loss: 4.8019\n","Epoch 12/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2258 - val_loss: 4.7962\n","Epoch 13/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2242 - val_loss: 4.8035\n","Epoch 14/100\n","1279/1279 [==============================] - 24s 18ms/step - loss: 6.2230 - val_loss: 4.7918\n","Epoch 15/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2214 - val_loss: 4.7984\n","Epoch 16/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2194 - val_loss: 4.7970\n","Epoch 17/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2162 - val_loss: 4.7943\n","Epoch 18/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2145 - val_loss: 4.7920\n","Epoch 19/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2134 - val_loss: 4.7906\n","Epoch 20/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2127 - val_loss: 4.7892\n","Epoch 21/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2105 - val_loss: 4.7811\n","Epoch 22/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2094 - val_loss: 4.7868\n","Epoch 23/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2087 - val_loss: 4.7873\n","Epoch 24/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2074 - val_loss: 4.7893\n","Epoch 25/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2049 - val_loss: 4.7868\n","Epoch 26/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2040 - val_loss: 4.7838\n","Epoch 27/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2026 - val_loss: 4.7828\n","Epoch 28/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2029 - val_loss: 4.7894\n","Epoch 29/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2016 - val_loss: 4.7853\n","Epoch 30/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2010 - val_loss: 4.7810\n","Epoch 31/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2002 - val_loss: 4.7848\n","Epoch 32/100\n","1279/1279 [==============================] - 24s 18ms/step - loss: 6.1987 - val_loss: 4.7792\n","Epoch 33/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1975 - val_loss: 4.7853\n","Epoch 34/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1966 - val_loss: 4.7810\n","Epoch 35/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1966 - val_loss: 4.7805\n","Epoch 36/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1947 - val_loss: 4.7794\n","Epoch 37/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1957 - val_loss: 4.7821\n","Epoch 38/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1939 - val_loss: 4.7813\n","Epoch 39/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1941 - val_loss: 4.7789\n","Epoch 40/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1933 - val_loss: 4.7779\n","Epoch 41/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1928 - val_loss: 4.7780\n","Epoch 42/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1921 - val_loss: 4.7792\n","Epoch 43/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1912 - val_loss: 4.7768\n","Epoch 44/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1913 - val_loss: 4.7771\n","Epoch 45/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1897 - val_loss: 4.7739\n","Epoch 46/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1899 - val_loss: 4.7750\n","Epoch 47/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1895 - val_loss: 4.7748\n","Epoch 48/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1893 - val_loss: 4.7723\n","Epoch 49/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1885 - val_loss: 4.7749\n","Epoch 50/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1879 - val_loss: 4.7732\n","Epoch 51/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1873 - val_loss: 4.7726\n","Epoch 52/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1871 - val_loss: 4.7742\n","Epoch 53/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1876 - val_loss: 4.7710\n","Epoch 54/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1864 - val_loss: 4.7735\n","Epoch 55/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1864 - val_loss: 4.7738\n","Epoch 56/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1850 - val_loss: 4.7726\n","Epoch 57/100\n","1279/1279 [==============================] - 24s 18ms/step - loss: 6.1852 - val_loss: 4.7720\n","Epoch 58/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1853 - val_loss: 4.7733\n","Epoch 59/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1848 - val_loss: 4.7724\n","Epoch 60/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1852 - val_loss: 4.7737\n","Epoch 61/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1851 - val_loss: 4.7753\n","Epoch 62/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1845 - val_loss: 4.7752\n","Epoch 63/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1851 - val_loss: 4.7744\n","Epoch 64/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1841 - val_loss: 4.7738\n","Epoch 65/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1839 - val_loss: 4.7731\n","Epoch 66/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1845 - val_loss: 4.7736\n","Epoch 67/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1830 - val_loss: 4.7731\n","Epoch 68/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1828 - val_loss: 4.7733\n","Epoch 69/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1825 - val_loss: 4.7741\n","Epoch 70/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1831 - val_loss: 4.7719\n","Epoch 71/100\n","1279/1279 [==============================] - 24s 18ms/step - loss: 6.1827 - val_loss: 4.7725\n","Epoch 72/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1829 - val_loss: 4.7720\n","Epoch 73/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1831 - val_loss: 4.7713\n","Epoch 74/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1828 - val_loss: 4.7705\n","Epoch 75/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1819 - val_loss: 4.7710\n","Epoch 76/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1821 - val_loss: 4.7713\n","Epoch 77/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1821 - val_loss: 4.7719\n","Epoch 78/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1814 - val_loss: 4.7713\n","Epoch 79/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1814 - val_loss: 4.7698\n","Epoch 80/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1817 - val_loss: 4.7695\n","Epoch 81/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1819 - val_loss: 4.7698\n","Epoch 82/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1812 - val_loss: 4.7706\n","Epoch 83/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1814 - val_loss: 4.7716\n","Epoch 84/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1811 - val_loss: 4.7715\n","Epoch 85/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1808 - val_loss: 4.7715\n","Epoch 86/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1809 - val_loss: 4.7720\n","Epoch 87/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1811 - val_loss: 4.7717\n","Epoch 88/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1806 - val_loss: 4.7720\n","Epoch 89/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1810 - val_loss: 4.7710\n","Epoch 90/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1801 - val_loss: 4.7701\n","Epoch 91/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1804 - val_loss: 4.7718\n","Epoch 92/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1808 - val_loss: 4.7711\n","Epoch 93/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1801 - val_loss: 4.7712\n","Epoch 94/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1805 - val_loss: 4.7709\n","Epoch 95/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1807 - val_loss: 4.7708\n","Epoch 96/100\n","1279/1279 [==============================] - 24s 18ms/step - loss: 6.1798 - val_loss: 4.7706\n","Epoch 97/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1802 - val_loss: 4.7702\n","Epoch 98/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1801 - val_loss: 4.7701\n","Epoch 99/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1801 - val_loss: 4.7715\n","Epoch 100/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1800 - val_loss: 4.7689\n","Best training loss: 6.179769992828369, Best validation loss: 4.7688798904418945\n","---------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["Training rnn model 2 out of 2 with seed 43\n","---------------------------------------\n","Epoch 1/100\n","1279/1279 [==============================] - 36s 19ms/step - loss: 6.3207 - val_loss: 4.8332\n","Epoch 2/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2771 - val_loss: 4.8148\n","Epoch 3/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2647 - val_loss: 4.8203\n","Epoch 4/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2570 - val_loss: 4.8132\n","Epoch 5/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2519 - val_loss: 4.8109\n","Epoch 6/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2466 - val_loss: 4.8141\n","Epoch 7/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2424 - val_loss: 4.8090\n","Epoch 8/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2380 - val_loss: 4.8115\n","Epoch 9/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2326 - val_loss: 4.7972\n","Epoch 10/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2300 - val_loss: 4.8001\n","Epoch 11/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2286 - val_loss: 4.7969\n","Epoch 12/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2255 - val_loss: 4.8010\n","Epoch 13/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2244 - val_loss: 4.7922\n","Epoch 14/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2224 - val_loss: 4.7954\n","Epoch 15/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2201 - val_loss: 4.8001\n","Epoch 16/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2191 - val_loss: 4.7990\n","Epoch 17/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2143 - val_loss: 4.7933\n","Epoch 18/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2130 - val_loss: 4.7906\n","Epoch 19/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2122 - val_loss: 4.7970\n","Epoch 20/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2111 - val_loss: 4.7877\n","Epoch 21/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2097 - val_loss: 4.7955\n","Epoch 22/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2089 - val_loss: 4.7890\n","Epoch 23/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2077 - val_loss: 4.7922\n","Epoch 24/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2059 - val_loss: 4.7859\n","Epoch 25/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2034 - val_loss: 4.7851\n","Epoch 26/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2028 - val_loss: 4.7828\n","Epoch 27/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2018 - val_loss: 4.7841\n","Epoch 28/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2010 - val_loss: 4.7863\n","Epoch 29/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.2007 - val_loss: 4.7859\n","Epoch 30/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1999 - val_loss: 4.7832\n","Epoch 31/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1987 - val_loss: 4.7785\n","Epoch 32/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1980 - val_loss: 4.7813\n","Epoch 33/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1961 - val_loss: 4.7807\n","Epoch 34/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1954 - val_loss: 4.7769\n","Epoch 35/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1951 - val_loss: 4.7821\n","Epoch 36/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1941 - val_loss: 4.7791\n","Epoch 37/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1932 - val_loss: 4.7785\n","Epoch 38/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1933 - val_loss: 4.7793\n","Epoch 39/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1928 - val_loss: 4.7763\n","Epoch 40/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1915 - val_loss: 4.7749\n","Epoch 41/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1907 - val_loss: 4.7729\n","Epoch 42/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1902 - val_loss: 4.7745\n","Epoch 43/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1898 - val_loss: 4.7725\n","Epoch 44/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1896 - val_loss: 4.7724\n","Epoch 45/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1891 - val_loss: 4.7728\n","Epoch 46/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1887 - val_loss: 4.7754\n","Epoch 47/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1887 - val_loss: 4.7736\n","Epoch 48/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1876 - val_loss: 4.7759\n","Epoch 49/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1871 - val_loss: 4.7708\n","Epoch 50/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1864 - val_loss: 4.7710\n","Epoch 51/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1866 - val_loss: 4.7706\n","Epoch 52/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1866 - val_loss: 4.7718\n","Epoch 53/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1856 - val_loss: 4.7708\n","Epoch 54/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1856 - val_loss: 4.7695\n","Epoch 55/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1850 - val_loss: 4.7736\n","Epoch 56/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1849 - val_loss: 4.7691\n","Epoch 57/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1849 - val_loss: 4.7690\n","Epoch 58/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1842 - val_loss: 4.7700\n","Epoch 59/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1846 - val_loss: 4.7740\n","Epoch 60/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1836 - val_loss: 4.7698\n","Epoch 61/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1837 - val_loss: 4.7707\n","Epoch 62/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1835 - val_loss: 4.7716\n","Epoch 63/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1829 - val_loss: 4.7685\n","Epoch 64/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1822 - val_loss: 4.7691\n","Epoch 65/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1825 - val_loss: 4.7679\n","Epoch 66/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1823 - val_loss: 4.7690\n","Epoch 67/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1824 - val_loss: 4.7682\n","Epoch 68/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1822 - val_loss: 4.7683\n","Epoch 69/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1815 - val_loss: 4.7688\n","Epoch 70/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1815 - val_loss: 4.7658\n","Epoch 71/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1821 - val_loss: 4.7667\n","Epoch 72/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1812 - val_loss: 4.7680\n","Epoch 73/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1814 - val_loss: 4.7676\n","Epoch 74/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1806 - val_loss: 4.7682\n","Epoch 75/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1806 - val_loss: 4.7691\n","Epoch 76/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1804 - val_loss: 4.7681\n","Epoch 77/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1811 - val_loss: 4.7672\n","Epoch 78/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1806 - val_loss: 4.7695\n","Epoch 79/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1808 - val_loss: 4.7679\n","Epoch 80/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1797 - val_loss: 4.7686\n","Epoch 81/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1801 - val_loss: 4.7673\n","Epoch 82/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1797 - val_loss: 4.7683\n","Epoch 83/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1801 - val_loss: 4.7677\n","Epoch 84/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1798 - val_loss: 4.7691\n","Epoch 85/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1809 - val_loss: 4.7667\n","Epoch 86/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1799 - val_loss: 4.7671\n","Epoch 87/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1797 - val_loss: 4.7673\n","Epoch 88/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1799 - val_loss: 4.7656\n","Epoch 89/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1797 - val_loss: 4.7681\n","Epoch 90/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1800 - val_loss: 4.7678\n","Epoch 91/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1794 - val_loss: 4.7685\n","Epoch 92/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1797 - val_loss: 4.7676\n","Epoch 93/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1794 - val_loss: 4.7692\n","Epoch 94/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1798 - val_loss: 4.7686\n","Epoch 95/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1789 - val_loss: 4.7677\n","Epoch 96/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1788 - val_loss: 4.7681\n","Epoch 97/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1791 - val_loss: 4.7673\n","Epoch 98/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1793 - val_loss: 4.7671\n","Epoch 99/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1788 - val_loss: 4.7680\n","Epoch 100/100\n","1279/1279 [==============================] - 24s 19ms/step - loss: 6.1787 - val_loss: 4.7674\n","Best training loss: 6.17865514755249, Best validation loss: 4.765636920928955\n","---------------------------------------\n","1/1 [==============================] - 1s 697ms/step\n","1/1 [==============================] - 1s 561ms/step\n","Ensemble Mean Absolute Error: 4.7658\n"]}],"id":"quC3VRDB78oo"},{"cell_type":"code","source":["#@title comments\n","\n","#vals (78+1):\n","# 1038/1038 [==============================] - 16s 15ms/step - loss: 6.2585 - val_loss: 5.8721\n","# Best training loss: 6.258157730102539, Best validation loss: 5.8713698387146\n","\n","# 1038/1038 [==============================] - 16s 16ms/step - loss: 6.2555 - val_loss: 5.8719\n","# Best training loss: 6.255491256713867, Best validation loss: 5.8713698387146\n","# ---------------------------------------\n","# 61/61 [==============================] - 1s 8ms/step\n","# 61/61 [==============================] - 1s 8ms/step\n","# Ensemble Mean Absolute Error: 5.8694\n","\n","#vals (73+1):\n","\n","#=================================================  258 feat  ==============================================\n","\n","#391-480 public-validation seed 42,43: lr:0.05,31 <sub,col 0.2,0.3>  depth = 14\n","#----------- [3281]\ttraining's l1: 6.10386\tvalid_1's l1: 5.86598\n","#----------- [2840]\ttraining's l1: 6.12476\tvalid_1's l1: 5.86619\n","#LGB Ensemble Mean Absolute Error: 5.8637\n","\n","\n","#391-480 public-validation seed 42,43: lr:0.005,511 <sub,col 0.2,0.3>  depth = 14\n","# ----------------- [5670]\ttraining's l1: 5.91513\tvalid_1's l1: 5.86133\n","#------------------ [4606]\ttraining's l1: 5.95538\tvalid_1's l1: 5.8615\n","# LGB Ensemble Mean Absolute Error: 5.8610\n","\n","\n","#391-480 public-validation seed 42,43: lr:0.005,1023 <sub,col 0.2,0.3>  depth = 14\n","# ----------------- [4028]\ttraining's l1: 5.79751\tvalid_1's l1: 5.86096\n","\n","\n","\n","\n","\n","#=================================================  253 feat  ==============================================\n","\n","#391-480 public-validation seed 42,43: lr:0.05,31 <sub,col 0.2,0.3>  depth = 14\n","#----------- [3058]\ttraining's l1: 6.11684\tvalid_1's l1: 5.8669\n","#----------- [3453]\ttraining's l1: 6.09743\tvalid_1's l1: 5.86741\n","#LGB Ensemble Mean Absolute Error: 5.8647\n","\n","\n","#391-480 public-validation seed 42,43: lr:0.005,511 <sub,col 0.2,0.3>  depth = 14\n","# ----------------- [5242]\ttraining's l1: 5.92851\tvalid_1's l1: 5.86238\n","#------------------ [6000]\ttraining's l1: 5.90175\tvalid_1's l1: 5.86203\n","\n","#LGB Ensemble Mean Absolute Error: 5.8618\n"],"metadata":{"papermill":{"duration":0.012549,"end_time":"2023-10-30T00:45:37.009605","exception":false,"start_time":"2023-10-30T00:45:36.997056","status":"completed"},"tags":[],"trusted":true,"id":"5Xvrt6pp78op","executionInfo":{"status":"ok","timestamp":1702482805490,"user_tz":300,"elapsed":24,"user":{"displayName":"Nima Shahbazi","userId":"15346950202304736225"}}},"execution_count":16,"outputs":[],"id":"5Xvrt6pp78op"},{"cell_type":"code","source":["#@title importance\n","imp = False\n","if imp:\n","    importances = lgb_model.feature_importance()\n","    feature_names = lgb_model.feature_name()\n","\n","    # Creating a DataFrame for easy manipulation\n","    importance_df = pd.DataFrame({\n","        'Feature': feature_names,\n","        'Importance': importances\n","    })\n","    importance_df.to_csv(\"importance.csv\")"],"metadata":{"trusted":true,"cellView":"form","id":"UR_NmIyX78op","executionInfo":{"status":"ok","timestamp":1702482805490,"user_tz":300,"elapsed":8,"user":{"displayName":"Nima Shahbazi","userId":"15346950202304736225"}}},"execution_count":17,"outputs":[],"id":"UR_NmIyX78op"},{"cell_type":"code","source":["#@title timeseries functions\n","\n","def clean_format(df):\n","    df['target'] = df['target'].astype('float64')\n","    for feat in ['stock_id','date_id','seconds_in_bucket']:\n","        df[feat] = df[feat].astype('int64')\n","    return df\n","\n","def clean_up(df):\n","    df = df[['stock_id','revealed_date_id','seconds_in_bucket','revealed_target']].rename(columns={'revealed_date_id': 'date_id', 'revealed_target': 'target'})\n","    df['target'].fillna(-0.06020069, inplace=True)\n","    df = clean_format(df)\n","    return df\n","\n","def manage_buffer(buffer, df, max_lag):\n","\n","    # df['iteration'] = df['iteration'].max()  # Ensure the iteration number is consistent in the new df chunk\n","    if buffer.empty:\n","        return df.copy()\n","    elif buffer['iteration'].nunique() < max_lag:\n","        return pd.concat([buffer, df], ignore_index=True)\n","    else:\n","        oldest_iteration = buffer['iteration'].min()\n","        buffer = buffer[buffer['iteration'] > oldest_iteration]\n","        return pd.concat([buffer, df], ignore_index=True)\n","\n","\n","def append_target_lags(df, target_buffer, lags):\n","\n","    column_to_lag = 'target'\n","    df_lagged = df.copy()\n","\n","    for lag in lags:\n","\n","        temp_df = target_buffer.copy()\n","        temp_df['date_id'] = temp_df['date_id'] + lag\n","\n","        temp_df.rename(columns={column_to_lag: f'lag{lag}_{column_to_lag}'}, inplace=True)\n","\n","        df_lagged = pd.merge(df_lagged, temp_df[['stock_id', 'date_id', 'seconds_in_bucket', f'lag{lag}_{column_to_lag}']],\n","                             on=['stock_id', 'date_id', 'seconds_in_bucket'],\n","                             how='left')\n","\n","    return df_lagged\n","\n","\n","def timeseries_lag_features(data, columns_to_lag, numbers_of_lag):\n","    for col in columns_to_lag:\n","        for lag in numbers_of_lag:\n","            data[f'{col}_lag{lag}'] = data.groupby(['stock_id'])[col].shift(lag)\n","    return data\n","\n","\n","def timeseries_diff_lag_features(data, columns_to_lag, numbers_of_lag):\n","    # Store new columns in a dictionary before joining them to the original DataFrame\n","    new_columns = {}\n","\n","    # Iterate over each group defined by 'stock_id' to reduce groupby operations\n","    grouped_data = data.groupby('stock_id')\n","\n","    for col in columns_to_lag:\n","        for lag in numbers_of_lag:\n","            diff_col_name = f'{col}_diff_lag{lag}'\n","\n","            # Compute the lagged difference for each group\n","            new_columns[diff_col_name] = data[col] - grouped_data[col].shift(lag)\n","\n","    # Concatenate all new columns and join with the original DataFrame\n","    new_columns_df = pd.DataFrame(new_columns)\n","    result_df = pd.concat([data, new_columns_df], axis=1)\n","\n","    return result_df\n","\n","\n","def timeseries_cumsum_features(df, columns_to_compute):\n","    df_copy = df.copy()\n","\n","    # Group by 'stock_id' and 'date_id' for cumulative sum calculation\n","    grouped = df_copy.groupby(['stock_id'])\n","\n","    # Calculate cumulative sum for each column within each group\n","    for column in columns_to_compute:\n","        cumsum_col_name = f'{column}_cumsum'\n","        df_copy[cumsum_col_name] = grouped[column].cumsum()\n","\n","    return df_copy\n","\n","\n","def timeseries_deviation_within_seconds(df, num_features):\n","    # Calculating the median for each numerical feature\n","    medians = df[num_features].median()\n","\n","    # Calculate deviations in a vectorized manner\n","    deviation_cols = {f'deviation_from_median_{feature}': df[feature] - medians[feature] for feature in num_features}\n","    df = df.assign(**deviation_cols)\n","\n","    return df\n","\n","\n","import psutil\n","\n","def memory_limit_exceeded(threshold_in_mb):\n","\n","    process = psutil.Process()\n","    current_memory_usage = process.memory_info().rss / (2**20)  # Convert to MB\n","    return current_memory_usage > threshold_in_mb"],"metadata":{"papermill":{"duration":0.022966,"end_time":"2023-10-30T00:45:37.057562","exception":false,"start_time":"2023-10-30T00:45:37.034596","status":"completed"},"tags":[],"trusted":true,"cellView":"form","id":"LlYf_VO_78op","executionInfo":{"status":"ok","timestamp":1702482805491,"user_tz":300,"elapsed":9,"user":{"displayName":"Nima Shahbazi","userId":"15346950202304736225"}}},"execution_count":18,"outputs":[],"id":"LlYf_VO_78op"},{"cell_type":"code","source":["if kaggle:\n","    import gc\n","    # remove train_small and train and clear the memory\n","    del train\n","    gc.collect()"],"metadata":{"papermill":{"duration":0.229017,"end_time":"2023-10-30T00:45:37.292673","exception":false,"start_time":"2023-10-30T00:45:37.063656","status":"completed"},"tags":[],"trusted":true,"id":"bKPY9kxN78oq","executionInfo":{"status":"ok","timestamp":1702482805491,"user_tz":300,"elapsed":9,"user":{"displayName":"Nima Shahbazi","userId":"15346950202304736225"}}},"execution_count":19,"outputs":[],"id":"bKPY9kxN78oq"},{"cell_type":"code","source":["#@title is inference\n","if is_inference:\n","\n","    if online_learning:\n","\n","        #lgb\n","        lgb_params['learning_rate'] = 0.001 #0.001\n","        lgb_params['n_estimators']  = 50    #10,20\n","        lgb_params['num_leaves']    = 127    #127\n","        lgb_params['device']        = 'cpu'\n","\n","        #nn\n","        online_nn_lr_rate    = 2e-4 #2e-4\n","        online_nn_batch_size = 16384 #16384\n","        online_nn_epochs     = 4     #2,4\n","\n","        #rnn\n","        online_rnn_lr_rate    = 2e-4 #2e-4\n","        online_rnn_batch_size = 2**12 #2**12 reduced for memory\n","        online_rnn_epochs     = 2      #2\n","\n","\n","\n","    if kaggle:\n","        import optiver2023\n","        env = optiver2023.make_env()\n","        iter_test = env.iter_test()\n","    else:\n","        env = MockApi()\n","        iter_test = env.iter_test()\n","\n","    if load_models:\n","\n","\n","\n","        if is_lgb:\n","            print(\"loading lgb models...\")\n","            lgb_models = []\n","            for i in range(num_models['lgb']):\n","                loaded_model = lgb.Booster(model_file=f'{models_path}model_lgb_{i}.txt')\n","                lgb_models.append(loaded_model)\n","\n","            lgb_features = lgb_models[0].feature_name()\n","            print(\"Done!\")\n","\n","\n","        if is_nn:\n","\n","            print(\"loading nn models...\")\n","            loaded_data          = load_pickle(f'{models_path}all_data.pkl')\n","            scaler               = loaded_data[\"scaler\"]\n","            medians              = loaded_data[\"medians\"]\n","            categorical_features = loaded_data[\"categorical_features\"]\n","            numerical_features   = loaded_data[\"numerical_features\"]\n","\n","            nn_models = []\n","            for i in range(num_models['nn']):\n","                loaded_model = load_model(f\"{models_path}swish_model_seed_{i}.h5\")\n","                nn_models.append(loaded_model)\n","            print(\"Done!\")\n","\n","        if is_rnn:\n","\n","            print(\"loading rnn models...\")\n","            loaded_data          = load_pickle(f'{models_path}rnn_all_data.pkl')\n","            rnn_scaler               = loaded_data[\"rnn_scaler\"]\n","            rnn_medians              = loaded_data[\"rnn_medians\"]\n","            rnn_categorical_features = loaded_data[\"rnn_categorical_features\"]\n","            rnn_numerical_features   = loaded_data[\"rnn_numerical_features\"]\n","\n","            rnn_models = []\n","            for i in range(num_models['rnn']):\n","                loaded_model = load_model(f\"{models_path}rnn_model_seed_{i}.h5\")\n","                rnn_models.append(loaded_model)\n","            print(\"Done!\")\n","\n","\n","\n","    #---------------------------buffer management ---------------------------\n","    max_target_lag  = num_of_target_lags\n","    i               = 0\n","    target_counter  = 0\n","    target_buffer   = pd.DataFrame()\n","    total_test      = pd.DataFrame()\n","    daily_online    = pd.DataFrame()\n","    #---------------------------buffer management ---------------------------\n","\n","\n","    for (test, revealed_targets, sample_prediction) in iter_test:\n","\n","\n","        test = convert_price_cols_float32(test)\n","\n","\n","        if test.seconds_in_bucket.iloc[0] == 0:\n","\n","            #----------------------------- storing previous targets -------------------------\n","            revealed_targets = clean_up(revealed_targets)\n","            revealed_targets['iteration'] = target_counter\n","            target_buffer = manage_buffer(target_buffer, revealed_targets, max_target_lag)\n","            target_counter += 1\n","\n","\n","            if manage_memory and memory_limit_exceeded(memory_threshold):\n","                online_learning = False\n","\n","\n","            if not daily_online.empty and online_learning:\n","\n","                daily_online.drop(columns='target', inplace=True)\n","                daily_online = pd.merge(daily_online, revealed_targets[['stock_id', 'date_id', 'seconds_in_bucket', 'target']],\n","                                            on=['stock_id', 'date_id', 'seconds_in_bucket'],\n","                                            how='inner')\n","\n","\n","                if is_lgb:\n","                    online_lgbs = []\n","\n","                    for lgb_model in lgb_models:\n","                        new_data = lgb.Dataset(daily_online[lgb_features], label=daily_online['target'])\n","                        online_lgb = lgb.train(lgb_params, new_data, init_model=lgb_model,valid_sets=[new_data,new_data],keep_training_booster=True,verbose_eval=1)\n","                        online_lgbs.append(online_lgb)\n","\n","                    #update the lgb_models\n","                    lgb_models = online_lgbs.copy()\n","\n","                if is_nn:\n","\n","                    X_online, y_online          = second_pass_for_nn(daily_online,numerical_features,categorical_features,is_inference=True)\n","\n","                    for nn_model in nn_models:\n","                        lr_schedule = ExponentialDecay(initial_learning_rate=online_nn_lr_rate,decay_steps=10000,decay_rate=0.96,staircase=True)\n","                        optimizer = Adam(learning_rate=lr_schedule)\n","                        nn_model.compile(optimizer=optimizer, loss = \"mean_absolute_error\")\n","                        nn_model.fit(X_online, y_online, epochs= online_nn_epochs, batch_size=online_nn_batch_size)\n","                        K.clear_session()\n","                        gc.collect()\n","\n","                if is_rnn:\n","\n","                    online_batches, online_targets = online_pass_for_rnn(daily_online, rnn_numerical_features, rnn_categorical_features, window_size)\n","\n","                    for rnn_model in rnn_models:\n","                        lr_schedule = ExponentialDecay( initial_learning_rate=online_rnn_lr_rate, decay_steps=10000, decay_rate=0.8,staircase=True)\n","                        optimizer = Adam(learning_rate=lr_schedule)\n","                        rnn_model.compile(optimizer=optimizer, loss = \"mean_absolute_error\")\n","                        rnn_model.fit(online_batches, online_targets, epochs=online_rnn_epochs, batch_size=online_rnn_batch_size)\n","                        K.clear_session()\n","                        gc.collect()\n","\n","            daily_online = pd.DataFrame()\n","            #----------------------------- storing previous targets -------------------------\n","\n","\n","        test['target'] = 0\n","\n","        #--------------- connected ------------\n","        test = feature_engineering(test)\n","        test = compute_imbalances(test, columns_sizes,prefix='_sz_')\n","        test = compute_imbalances(test, columns_prices,prefix = '_pr_')\n","\n","        test = append_target_lags(test, target_buffer, target_lags)\n","\n","        test = calculate_stat_lag(test, num_lags=num_of_target_lags)\n","\n","        eng_features       = [feature for feature in test.columns if \"_eng\" in feature]\n","        imb_features_all   = [feature for feature in test.columns if \"_imb_\" in feature]\n","        imb_features_price = [feature for feature in test.columns if \"_pr_\" in feature]\n","        imb_features_size  = [feature for feature in test.columns if \"_sz_\" in feature]\n","        #--------------- connected ------------\n","\n","        #------------------------------------  this for deviation within seconds  -------------------------\n","        deviation_cols = raw_cols + eng_features + imb_features_size + imb_features_price\n","        test = timeseries_deviation_within_seconds(test,deviation_cols)\n","\n","        test = map_global(test,aggregated_dic)\n","\n","        #------------------------ get the full data up to current seconds in bucket in date-----------------------------------\n","        if not daily_online.empty:\n","            full_data = pd.concat([daily_online[test.columns], test], ignore_index=True)\n","        else:\n","            full_data = test\n","\n","\n","        diff_lag_cols = raw_cols + eng_features\n","        full_data = timeseries_diff_lag_features(full_data, diff_lag_cols, diff_lags)\n","\n","        cumsum_columns = columns_sizes + imb_features_size + eng_features\n","        full_data = timeseries_cumsum_features(full_data, cumsum_columns)\n","\n","        #------------------------ get the full data up to current seconds in bucket in date-----------------------------------\n","\n","\n","        # getting back to test size\n","        test = full_data.iloc[-test.shape[0]:].copy()\n","\n","\n","        #------------------------------------  Global based on stock_id ---------------------------------------\n","        test.replace([np.inf, -np.inf], np.nan, inplace=True)\n","\n","\n","        if test.currently_scored.iloc[0]:\n","            if is_lgb:\n","                test['target'] = ens_models[0]*make_predictions(lgb_models,test[lgb_features],model='lgb')\n","\n","            if is_nn:\n","                X_test, _           = second_pass_for_nn(test,numerical_features,categorical_features,is_inference=True)\n","                test['target'] += ens_models[1]*make_predictions(nn_models, X_test,model='nn').flatten()\n","\n","            if is_rnn:\n","                X_test, _           = second_pass_for_rnn(full_data, rnn_numerical_features, rnn_categorical_features, window_size, is_inference=True)\n","                test['target'] += ens_models[2]*make_predictions(rnn_models, X_test,model='nn').flatten()\n","\n","\n","\n","        if not kaggle:\n","            total_test = pd.concat([total_test, test], ignore_index=True)\n","\n","\n","        daily_online = pd.concat([daily_online, test], ignore_index=True)\n","\n","\n","        sample_prediction = pd.merge(sample_prediction.drop(columns='target'), test[['row_id', 'target']], on=['row_id'], how='left')\n","        sample_prediction['target'].fillna(0, inplace=True)\n","        # sample_prediction['target'].replace([np.inf, -np.inf], 0, inplace=True)\n","\n","        env.predict(sample_prediction)\n","        i += 1"],"metadata":{"papermill":{"duration":128.751281,"end_time":"2023-10-30T00:47:46.050532","exception":false,"start_time":"2023-10-30T00:45:37.299251","status":"completed"},"tags":[],"trusted":true,"cellView":"form","id":"DVpNJANE78oq","executionInfo":{"status":"ok","timestamp":1702482805491,"user_tz":300,"elapsed":8,"user":{"displayName":"Nima Shahbazi","userId":"15346950202304736225"}}},"execution_count":20,"outputs":[],"id":"DVpNJANE78oq"},{"cell_type":"code","source":["#@title check inference\n","\n","if  is_inference and not kaggle:\n","\n","    last_date = True\n","    if last_date:\n","        original = test_data.query(\"date_id >= 480\").reset_index()\n","        loop_test = total_test.query(\"date_id >= 480\").reset_index()\n","    else:\n","        original = test_data\n","        loop_test = total_test\n","\n","\n","    if is_lgb:\n","        original['preds'] =  make_predictions(lgb_models, original[features],model='lgb')\n","        mae = mean_absolute_error(original['target'], original['preds'])\n","        print(f\"Mean Absolute Error lgb: {mae:.4f}\")\n","\n","    if is_nn:\n","        X_test, y_test  = second_pass_for_nn(original,numerical_features,categorical_features)\n","        predictions =  make_predictions(nn_models, X_test,model='nn')\n","        print(f\"Mean Absolute Error nn: {mean_absolute_error(y_test, predictions):.4f}\")\n","\n","    if is_rnn:\n","        X_test, y_test  = second_pass_for_rnn(original,rnn_numerical_features,rnn_categorical_features,window_size)\n","        predictions =  make_predictions(rnn_models, X_test,model='nn')\n","        print(f\"Mean Absolute Error rnn: {mean_absolute_error(y_test, predictions):.4f}\")\n","\n","    mae = mean_absolute_error(original['target'], loop_test['target'])\n","    print(f\"Mean Absolute Error on loop inference : {mae:.4f}\")\n","\n","\n","#0.05, 31 full vaidation Mean Absolute Error lgb on origin data 480: 4.7701\n","# # test discrepencies\n","# common_columns = loop_test.columns.intersection(original.columns)\n","\n","# # Step 2: Reorder columns in both DataFrames\n","# loop_test_common = loop_test[common_columns]\n","# original_common = original[common_columns]\n","# loop_test_common.describe().to_csv('loop.csv')\n","# original_common.describe().to_csv(\"original.csv\")"],"metadata":{"papermill":{"duration":0.203249,"end_time":"2023-10-30T00:47:46.439280","exception":false,"start_time":"2023-10-30T00:47:46.236031","status":"completed"},"tags":[],"trusted":true,"cellView":"form","id":"DggnWEpx78oq","executionInfo":{"status":"ok","timestamp":1702482805491,"user_tz":300,"elapsed":8,"user":{"displayName":"Nima Shahbazi","userId":"15346950202304736225"}}},"execution_count":21,"outputs":[],"id":"DggnWEpx78oq"},{"cell_type":"code","source":[],"metadata":{"id":"8ai-6TrA78oq","executionInfo":{"status":"ok","timestamp":1702482805491,"user_tz":300,"elapsed":8,"user":{"displayName":"Nima Shahbazi","userId":"15346950202304736225"}}},"execution_count":21,"outputs":[],"id":"8ai-6TrA78oq"},{"cell_type":"code","source":["#@title simulation\n","import math\n","\n","def online_pass_for_rnn_sim(df, rnn_numerical_features, rnn_categorical_features, window_size):\n","    # Check if the DataFrame is empty\n","    global rnn_scaler,rnn_medians\n","\n","    if df.empty:\n","        return None, None\n","\n","    # Work on a copy of the DataFrame to avoid changing the original df\n","    df_copy = df.copy()\n","\n","    # Preprocess Data\n","    df_copy['seconds_in_bucket'] = df_copy['seconds_in_bucket'] / 10\n","    df_copy['imbalance_buy_sell_flag'] += 1\n","\n","\n","    grouped = df_copy.groupby(['stock_id'])\n","    all_batches = []\n","    all_targets = []\n","\n","    for _, group in tqdm(grouped, desc=\"Processing groups\"):\n","        # Precompute sequences for the current group\n","        precomputed_data = precompute_sequences(group, window_size, rnn_numerical_features, rnn_categorical_features)\n","\n","        # Initialize containers for group sequences and targets\n","        group_sequences = []\n","        group_targets = []\n","\n","        # Iterate over the time steps and retrieve precomputed sequences\n","        for time_step in range(55):\n","            sequence, target = get_sequence(precomputed_data, time_step)\n","            if sequence.size > 0:\n","                group_sequences.append(sequence)\n","                group_targets.append(target)\n","\n","        # Extend the main batches with the group's sequences and targets\n","        all_batches.extend(group_sequences)\n","        all_targets.extend(group_targets)\n","\n","    df_batches = np.array(all_batches)\n","    df_targets = np.array(all_targets)\n","\n","\n","    return df_batches, df_targets\n","\n","\n","\n","def second_pass_for_rnn_sim(df, rnn_numerical_features, rnn_categorical_features, window_size):\n","    # Check if the DataFrame is empty\n","    global rnn_scaler,rnn_medians\n","\n","    if df.empty:\n","        return None, None\n","\n","    # Work on a copy of the DataFrame to avoid changing the original df\n","    df_copy = df.copy()\n","\n","    # Preprocess Data\n","    df_copy['seconds_in_bucket'] = df_copy['seconds_in_bucket'] / 10\n","    df_copy['imbalance_buy_sell_flag'] += 1\n","\n","\n","    df_copy_batches, df_copy_targets = create_last_batches(df_copy, window_size, rnn_numerical_features, rnn_categorical_features)\n","\n","    df_copy_batches = np.array(df_copy_batches)\n","    df_copy_targets = np.array(df_copy_targets)\n","\n","\n","    return df_copy_batches, df_copy_targets\n","\n","if simulation:\n","\n","    online_learning= True\n","\n","    #-------------------LGB----------------\n","    initial_learning_rates = [0.001]\n","    n_estimators_values    = [20]\n","    num_leaves_values      = [127]\n","    max_depth_values       = [14]\n","    #-------------------LGB----------------\n","\n","\n","    #-------------------NN----------------\n","    nn_init_rate_values = [1e-4]#,2e-4,1e-5,5e-4]\n","    online_batch_size_values =[16384]\n","    online_epochs_values = [2]#4,8,10]\n","    #-------------------NN----------------\n","\n","    #-------------------RNN----------------\n","    rnn_init_rate_values = [1e-4,2e-4]\n","    rnn_online_batch_size_values =[2**10,2**11,2**12,2*9]\n","    rnn_online_epochs_values = [2,4,8,10]\n","    #-------------------RNN----------------\n","\n","    br_count = 200\n","\n","    results = []\n","\n","    models_path = simulation_path\n","# Nested loop over all parameter combinations\n","\n","    for init_lr in initial_learning_rates:\n","        for n_estimators in n_estimators_values:\n","            for num_leaves in num_leaves_values:\n","                    for max_depth in max_depth_values:\n","\n","                        for nn_init_rate in nn_init_rate_values:\n","                            for online_batch_size in online_batch_size_values:\n","                                for online_epochs in online_epochs_values:\n","                                    for rnn_init_rate in rnn_init_rate_values:\n","                                        for rnn_online_batch_size in rnn_online_batch_size_values:\n","                                            for rnn_online_epochs in rnn_online_epochs_values:\n","\n","                                                if is_nn:\n","                                                    print(\"loading nn models...\")\n","                                                    loaded_data          = load_pickle(f'{models_path}all_data.pkl')\n","                                                    scaler               = loaded_data[\"scaler\"]\n","                                                    medians              = loaded_data[\"medians\"]\n","                                                    categorical_features = loaded_data[\"categorical_features\"]\n","                                                    numerical_features   = loaded_data[\"numerical_features\"]\n","\n","                                                    nn_models = []\n","                                                    for i in range(1):\n","                                                        loaded_model = load_model(f\"{models_path}swish_model_seed_{i}.h5\")\n","                                                        nn_models.append(loaded_model)\n","                                                    print(\"Done!\")\n","\n","                                                    nn_models_original = []\n","                                                    for i in range(1):\n","                                                        loaded_model_original = load_model(f\"{models_path}swish_model_seed_{i}.h5\")\n","                                                        nn_models_original.append(loaded_model_original)\n","                                                    print(\"Done!\")\n","\n","                                                if is_rnn:\n","                                                    print(\"loading rnn models...\")\n","                                                    loaded_data          = load_pickle(f'{models_path}rnn_all_data.pkl')\n","                                                    rnn_scaler               = loaded_data[\"rnn_scaler\"]\n","                                                    rnn_medians              = loaded_data[\"rnn_medians\"]\n","                                                    rnn_categorical_features = loaded_data[\"rnn_categorical_features\"]\n","                                                    rnn_numerical_features   = loaded_data[\"rnn_numerical_features\"]\n","\n","                                                    rnn_models = []\n","                                                    for i in range(1):\n","                                                        loaded_model = load_model(f\"{models_path}rnn_model_seed_{i}.h5\")\n","                                                        rnn_models.append(loaded_model)\n","                                                    print(\"Done!\")\n","\n","                                                    rnn_models_original = []\n","                                                    for i in range(1):\n","                                                        loaded_model_original = load_model(f\"{models_path}rnn_model_seed_{i}.h5\")\n","                                                        rnn_models_original.append(loaded_model_original)\n","                                                    print(\"Done!\")\n","\n","                                                if is_lgb:\n","                                                    print(\"loading lgb models...\")\n","                                                    lgb_models_original = []\n","                                                    for i in range(1):\n","                                                        loaded_model = lgb.Booster(model_file=f'{models_path}model_lgb_{i}.txt')\n","                                                        lgb_models_original.append(loaded_model)\n","\n","\n","\n","\n","                                                    online_models = []\n","                                                    for i in range(1):\n","                                                        loaded_model = lgb.Booster(model_file=f'{models_path}model_lgb_{i}.txt')\n","                                                        online_models.append(loaded_model)\n","\n","                                                    lgb_features = online_models[0].feature_name()\n","\n","                                                print(\"making prediction on the whole test data....\")\n","                                                preds = make_predictions(lgb_models_original,test_data[lgb_features],model='lgb')\n","                                                mae_original = mean_absolute_error(test_data['target'],preds)\n","                                                print(f\"lgb Score on Test: {mae_original}\")\n","\n","\n","\n","\n","                                                # Set the parameters for your model\n","                                                lgb_params['n_estimators'] = n_estimators\n","                                                lgb_params['num_leaves'] = num_leaves\n","                                                lgb_params['max_depth'] = max_depth\n","                                                lgb_params['learning_rate'] = init_lr\n","\n","                                                mae_values = []\n","                                                loop_test = pd.DataFrame()\n","                                                subset_cum = pd.DataFrame()\n","                                                counter = 0\n","\n","                                                for date_id in tqdm(test_data['date_id'].unique(), desc=\"Iterating over unique date_ids\"):\n","\n","\n","                                                    counter +=1\n","                                                    #-------------------updating lr decay-------------------\n","                                                    subset_test = test_data[test_data['date_id'] == date_id]\n","\n","                                                    if is_lgb:\n","                                                        subset_test['online_preds']   = make_predictions(online_models,subset_test[lgb_features],model='lgb')\n","                                                        subset_test['original_preds'] = make_predictions(lgb_models_original,subset_test[lgb_features],model='lgb')\n","\n","                                                    if is_nn:\n","                                                        X_test, _           = second_pass_for_nn(subset_test,numerical_features,categorical_features)\n","                                                        subset_test['online_preds']  = make_predictions(nn_models, X_test,model='nn').flatten()\n","                                                        subset_test['original_preds'] = make_predictions(nn_models_original, X_test,model='nn').flatten()\n","\n","                                                    if is_rnn:\n","                                                        X_test, _           = online_pass_for_rnn_sim(subset_test,rnn_numerical_features,rnn_categorical_features,window_size)\n","                                                        subset_test['online_preds']  = make_predictions(rnn_models, X_test,model='nn').flatten()\n","                                                        subset_test['original_preds'] = make_predictions(rnn_models_original, X_test,model='nn').flatten()\n","\n","\n","\n","                                                    mae_online = mean_absolute_error(subset_test['target'], subset_test['online_preds'])\n","                                                    mae_original = mean_absolute_error(subset_test['target'], subset_test['original_preds'])\n","\n","                                                    print(f\"-------------------------------------------- Online: Mean Absolute Error lgb: {mae_online:.5f}\")\n","                                                    print(f\"-------------------------------------------original: Mean Absolute Error lgb: {mae_original:.5f}\")\n","\n","\n","                                                    mae_values.append({'mae_online': mae_online, 'mae_original': mae_original})\n","\n","                                                    loop_test = pd.concat([loop_test, subset_test], ignore_index=True)\n","                                                    subset_cum = pd.concat([subset_cum, subset_test], ignore_index=True)\n","\n","                                                    if counter >=br_count:\n","                                                        break\n","\n","                                                    if online_learning and subset_cum.date_id.nunique() >= 1:\n","\n","                                                        if is_lgb:\n","                                                            print(f\"==================learning rate = {lgb_params['learning_rate']}==================\")\n","                                                            #----------------------------------------   LGB online learning   ----------------------------------------\n","                                                            temp_lgbs = []\n","                                                            for lgb_model in online_models:\n","                                                                new_data = lgb.Dataset(subset_test[lgb_features], label=subset_test['target'],free_raw_data=False)\n","                                                                temp_lgb = lgb.train(lgb_params, new_data, init_model=lgb_model,valid_sets=[new_data,new_data],keep_training_booster=True,verbose_eval=5)\n","                                                                temp_lgbs.append(temp_lgb)\n","\n","\n","\n","                                                            online_models = temp_lgbs.copy()\n","                                                            #----------------------------------------   LGB online learning   ----------------------------------------\n","                                                        if is_nn:\n","\n","                                                            print(\"updating NN model....\")\n","                                                            for nn_model in nn_models:\n","\n","                                                                # for layer in nn_model.layers[:-5]:\n","                                                                #     layer.trainable = False\n","\n","                                                                lr_schedule = ExponentialDecay( initial_learning_rate=nn_init_rate, decay_steps=10000, decay_rate=0.9,staircase=True)\n","                                                                optimizer = Adam(learning_rate=lr_schedule)\n","                                                                nn_model.compile(optimizer=optimizer, loss = \"mean_absolute_error\")\n","                                                                X_online, y_online    = second_pass_for_nn(subset_test,numerical_features,categorical_features)\n","                                                                history = nn_model.fit(X_online, y_online, epochs=online_epochs, batch_size=online_batch_size)\n","\n","                                                        if is_rnn:\n","\n","                                                            online_batches, online_targets = online_pass_for_rnn_sim(subset_test, rnn_numerical_features, rnn_categorical_features, window_size)\n","\n","                                                            for rnn_model in rnn_models:\n","                                                                lr_schedule = ExponentialDecay( initial_learning_rate=rnn_init_rate, decay_steps=10000, decay_rate=0.8,staircase=True)\n","                                                                optimizer = Adam(learning_rate=lr_schedule)\n","                                                                rnn_model.compile(optimizer=optimizer, loss = \"mean_absolute_error\")\n","                                                                rnn_model.fit(online_batches, online_targets, epochs=rnn_online_epochs, batch_size=rnn_online_batch_size)\n","\n","\n","\n","\n","                                                        subset_cum  = pd.DataFrame()\n","\n","\n","                                                mae_online = mean_absolute_error(loop_test['target'], loop_test['online_preds'])\n","                                                mae_original = mean_absolute_error(loop_test['target'], loop_test['original_preds'])\n","\n","                                                print(f\"Total Online: Mean Absolute Error lgb: {mae_online:.5f}\")\n","                                                print(f\"Total Original: Mean Absolute Error lgb: {mae_original:.5f}\")\n","\n","\n","                                                loop_test_418 =  loop_test.query(f\"date_id >= 418\").reset_index().copy()\n","                                                mae_online_418 = mean_absolute_error(loop_test_418['target'], loop_test_418['online_preds'])\n","                                                mae_original_418 = mean_absolute_error(loop_test_418['target'], loop_test_418['original_preds'])\n","\n","                                                results.append({\n","                                                    'initial_learning_rate': init_lr,\n","                                                    'n_estimators': n_estimators,\n","                                                    'num_leaves': num_leaves,\n","                                                    'max_depth':max_depth,\n","\n","                                                    'nn_init_rate':nn_init_rate,\n","                                                    'online_batch_size':online_batch_size,\n","                                                    'online_epochs':online_epochs,\n","\n","                                                    'rnn_init_rate':rnn_init_rate,\n","                                                    'rnn_online_batch_size':rnn_online_batch_size,\n","                                                    'rnn_online_epochs':rnn_online_epochs,\n","\n","                                                    'mae_online': mae_online,\n","                                                    'mae_original': mae_original,\n","                                                    'mae_online_418_end': mae_online_418,\n","                                                    'mae_original_418_end': mae_original_418\n","                                                })\n","                                                pd.DataFrame(results).to_csv(\"iter_rnn_result_online.csv\", index=False)\n","\n","\n","\n","\n","    results_df = pd.DataFrame(results)\n","    results_df.to_csv(\"final_result_rnn_online.csv\")\n","\n","\n","    loop_test_418 =  loop_test.query(f\"date_id >= 418\").reset_index().copy()\n","    mae_online_418 = mean_absolute_error(loop_test_418['target'], loop_test_418['online_preds'])\n","    mae_original_418 = mean_absolute_error(loop_test_418['target'], loop_test_418['original_preds'])\n","\n","    print(f\"Total Online: Mean Absolute Error lgb on 418-end : {mae_online_418:.5f}\")\n","    print(f\"Total Original: Mean Absolute Error lgb on 418-end: {mae_original_418:.5f}\")\n","\n","# Total Online: Mean Absolute Error lgb on 418-end : 5.65682\n","# Total Original: Mean Absolute Error lgb on 418-end: 5.65876\n"],"metadata":{"trusted":true,"cellView":"form","id":"QXWSY---78oq","executionInfo":{"status":"ok","timestamp":1702482805491,"user_tz":300,"elapsed":8,"user":{"displayName":"Nima Shahbazi","userId":"15346950202304736225"}}},"execution_count":22,"outputs":[],"id":"QXWSY---78oq"},{"cell_type":"code","source":[],"metadata":{"id":"WShhzgK478or","executionInfo":{"status":"ok","timestamp":1702482805491,"user_tz":300,"elapsed":7,"user":{"displayName":"Nima Shahbazi","userId":"15346950202304736225"}}},"execution_count":22,"outputs":[],"id":"WShhzgK478or"}]}